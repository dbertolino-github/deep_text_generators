{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - Importing needed dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Declaring global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "SENTENCES_MAX_LENGTH = 30\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_TRAIN_PLOTS = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Text cleaning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "\n",
    "    text = re.sub(\"[@#$+%*:()\\\"-]\", ' ', text)\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    text = text.replace(\",\", \" ,\")\n",
    "    text = text.replace(\"!\", \" !\")\n",
    "    text = text.replace(\"?\", \" ?\")\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Try read data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_read_data():\n",
    "    '''\n",
    "    '''  \n",
    "    try:\n",
    "        dirname = os.path.abspath('')\n",
    "        filepath = os.path.join(dirname, 'input_data/wiki_movie_plots.csv')\n",
    "        print('1 - READING FILM PLOTS ...')\n",
    "        dataframe = pd.read_csv(filepath, sep=',')\n",
    "        plotsList = dataframe['Plot']\n",
    "        print('{} plots imported.'.format(len(plotsList)))\n",
    "        plotsList = plotsList[:NUM_TRAIN_PLOTS]\n",
    "        print('2 - CLEANING TEXT ...')\n",
    "        for idx, p in enumerate(plotsList):\n",
    "            plotsList[idx] = clean(p)\n",
    "        trainPlotsList = plotsList[:NUM_TRAIN_PLOTS]    \n",
    "\n",
    "    except IOError:\n",
    "        sys.exit('Cannot find data!')\n",
    "    \n",
    "    return plotsList, trainPlotsList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(plots_list, train_plots_list, batch_size=BATCH_SIZE): \n",
    "    '''\n",
    "    '''\n",
    "    print('3 - PREPROCESSING SENTECES')\n",
    " \n",
    "    # EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "    maxLength = SENTENCES_MAX_LENGTH\n",
    "    inputSentences = []\n",
    "    targetSentences = []\n",
    "    outputSentences = []\n",
    "\n",
    "    for plot in train_plots_list:\n",
    "        words = plot.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+maxLength] for i in range(0, len(words), maxLength)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "                \n",
    "    \n",
    "    numSamples = len(inputSentences)\n",
    "    print('Num samples: {}'.format(numSamples))\n",
    "    stepsPerEpoch = numSamples//batch_size\n",
    "    print('StepsPerEpoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "    # CREATE VOCABULARY OF WORDS\n",
    "    idx2word = ['<PAD>','<START>', '<END>']\n",
    "    for plot in plotsList:\n",
    "\n",
    "        words = plot.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        for word in words:\n",
    "            if word not in idx2word:\n",
    "                idx2word.append(word)\n",
    "\n",
    "    word2idx = {}\n",
    "    for word in idx2word:\n",
    "        word2idx[word] = len(word2idx)\n",
    "    \n",
    "    vocabLength = len(idx2word)\n",
    "    print('Vocabulary Size: {}'.format(vocabLength))\n",
    "\n",
    "    # WRITE DATASET TO TXT  \n",
    "    train_dataset = []\n",
    "\n",
    "    print(\"Creating dataset to feed Model . . . \")\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))  \n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath) \n",
    "    \n",
    "    d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "    df = pd.DataFrame(data=d) \n",
    "    df = shuffle(df)\n",
    "    df.to_csv(filePath, index=False)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return idx2word, vocabLength, word2idx, stepsPerEpoch, numSamples, maxLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Data generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        index = 0\n",
    "        for idx, row in df.iterrows():\n",
    "        \n",
    "            if index >= batch_size:\n",
    "                break\n",
    "\n",
    "            if counter >= numSamples:\n",
    "                break\n",
    "\n",
    "            encoderTokens = row['input_encoder'].split(' ')\n",
    "            decoderTokens = row['input_decoder'].split(' ')\n",
    "            outputTokens = row['output_decoder'].split(' ')\n",
    "            \n",
    "            b = True\n",
    "            while b:\n",
    "                if('' in encoderTokens): encoderTokens.remove('')\n",
    "                else: b = False\n",
    "\n",
    "            b = True\n",
    "            while b:\n",
    "                if('' in decoderTokens): decoderTokens.remove('')\n",
    "                else: b = False\n",
    "\n",
    "            encoderInputData = np.zeros((1, max_length + 2), dtype='int')\n",
    "            decoderInputData = np.zeros((1, max_length + 2), dtype='int')\n",
    "            decoderTargetData = np.zeros((1, max_length + 2, 1),dtype='int')\n",
    "            \n",
    "            for t, word in enumerate(encoderTokens):\n",
    "                encoderInputData[0, t] = word_2_idx[word]\n",
    "            for t, word in enumerate(decoderTokens):\n",
    "                decoderInputData[0, t] = word_2_idx[word]\n",
    "            for t, word in enumerate(outputTokens):\n",
    "                # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "                decoderTargetData[0, t, 0] = word_2_idx[word]\n",
    "                \n",
    "            df.drop(df.index[[idx]])\n",
    "            index = index + 1\n",
    "            counter = counter + 1\n",
    "            \n",
    "            yield([encoderInputData,decoderInputData], decoderTargetData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Generation function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sentences, model, max_length, word_2_idx, idx_2_word):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    decoded_sentences = []\n",
    "    \n",
    "    for s in sentences:\n",
    "\n",
    "        print('Generating from: {}'.format(s))\n",
    "        encoderTokens = []\n",
    "        s = clean(s)\n",
    "        encoderwords = s.split(' ')\n",
    "        for w in encoderwords:\n",
    "            encoderTokens.append(word2idx[w])\n",
    "        encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "        encoderInputData = np.zeros((1, max_length + 2), dtype='int64')\n",
    "        \n",
    "        decoded = decode(\n",
    "        model,\n",
    "        encoderTokens,\n",
    "        start_token=word2idx['<START>'],\n",
    "        end_token=word2idx['<END>'],\n",
    "        pad_token=word2idx['<PAD>'],\n",
    "        max_len=maxLength,\n",
    "        )\n",
    "\n",
    "        decodedPhrase = ''\n",
    "        for x in decoded:\n",
    "            decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "        \n",
    "        decoded_sentences.append(decodedPhrase)\n",
    "        print('Generated: {}'.format(decodedPhrase))\n",
    "    \n",
    "    return decoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - Logic__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - READING FILM PLOTS ...\n",
      "34886 plots imported.\n",
      "2 - CLEANING TEXT ...\n",
      "3 - PREPROCESSING SENTECES\n",
      "Num samples: 849258\n",
      "StepsPerEpoch: 6634\n",
      "Vocabulary Size: 32588\n",
      "Creating dataset to feed Model . . . \n",
      "Done.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 128),  4171264     Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 128)    263296      Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 128)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 128)    263296      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 128)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 32588)  32588       Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 4,929,868\n",
      "Trainable params: 758,604\n",
      "Non-trainable params: 4,171,264\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      "6634/6634 [==============================] - 800s 121ms/step - loss: 2.5940\n",
      "Generating from: on a beautiful summer day three people\n",
      "Generated:  <START> the kidnapping , while the ship with his granddaughter sally . the hope of finding a train owned by traveling <END>\n",
      " <START> the kidnapping , while the ship with his granddaughter sally . the hope of finding a train owned by traveling <END>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "plotsList, trainPlotsList = try_read_data()\n",
    "idx2word, vocabLength, word2idx, stepsPerEpoch, numSamples, maxLength = preprocess(\n",
    "    plots_list=plotsList,\n",
    "    train_plots_list=trainPlotsList\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=1,\n",
    "    decoder_num=1,\n",
    "    head_num=16,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((len(word2idx), EMBEDDING_DIM)),\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    trainGen = data_generator(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=maxLength, \n",
    "            vocab_length=vocabLength\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit_generator(\n",
    "            trainGen,\n",
    "            steps_per_epoch=stepsPerEpoch,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    decoded_sentences = generate_text(\n",
    "        sentences = ['on a beautiful summer day three people'], \n",
    "        model = model, \n",
    "        max_length = maxLength, \n",
    "        word_2_idx = word2idx,\n",
    "        idx_2_word = idx2word\n",
    "    )\n",
    "    \n",
    "    print(decoded_sentences[0])\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: On a beautiful summer day three people\n",
      "Generated:  <START> the kidnapping , while the ship with his granddaughter sally . the hope of finding a train owned by traveling <END>\n",
      "Generating from: The terrified servant leaves the rifle\n",
      "Generated:  <START> the kidnapping , while the ship with his granddaughter sally . the hope of finding a train owned by traveling <END>\n",
      "Generating from: The president is in trouble\n",
      "Generated:  <START> the kidnapping , while the ship with his granddaughter sally . the hope of finding a surprised and never revealed the cover of her father <END>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "resultsModelPatht = os.path.join(dirname, 'output_data/tr_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = ['On a beautiful summer day three people', \n",
    "    'The terrified servant leaves the rifle' , \n",
    "    'The president is in trouble']\n",
    "\n",
    "decoded_sentences = generate_text(\n",
    "    sentences = sentences,\n",
    "    model = model, \n",
    "    max_length = maxLength,\n",
    "    word_2_idx = word2idx, \n",
    "    idx_2_word = idx2word, \n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPatht, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
