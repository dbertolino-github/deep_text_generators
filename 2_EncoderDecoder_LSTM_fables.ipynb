{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import dependencies and decleare global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from copy import deepcopy\n",
    "from keras.models import load_model\n",
    "\n",
    "EPOCHS = 1\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    cleanedFables = []\n",
    "    for f in fables:\n",
    "        cleanedFables.append(clean(f))\n",
    "    \n",
    "    print('{} fables cleaned.'.format(len(cleanedFables)))\n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen = 0\n",
    "for f in cleanedFables:\n",
    "    words = f.split(' ')\n",
    "    sumLen += len(words)\n",
    "\n",
    "avgLen = sumLen/len(cleanedFables)\n",
    "avgLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "wordSequence = []\n",
    "for fable in cleanedFables:\n",
    "    words = fable.split(' ')\n",
    "    wordSequence.extend(words)\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "textAsInt = np.array([word2idx[w] for w in wordSequence])\n",
    "vocab_size = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "for fable in cleanedFables:\n",
    "        words = fable.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+MAX_LENGTH] for i in range(0, len(words), MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_fables_{}_{}.csv'.format(\n",
    "MAX_LENGTH,  \n",
    "BATCH_SIZE)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_fables_{}_{}.csv'.format(\n",
    "    MAX_LENGTH,  \n",
    "    BATCH_SIZE)))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "        if(i%10000 == 0):print('Generating feeding data... {}/{}'.format(i,numSamples))\n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define function to build the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(vocab_length, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Define an input sequence and process it.\n",
    "    # Input layer of the encoder :\n",
    "    encoderInput = Input(shape=(None,))\n",
    "    \n",
    "    # Hidden layers of the encoder :\n",
    "    encoder_embedding = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])(encoderInput)\n",
    "\n",
    "    # Output layer of the encoder :\n",
    "    encoder_LSTM = CuDNNLSTM(hidden_dim , return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoderStates = [state_h, state_c]\n",
    "    \n",
    "    \n",
    "    return encoderInput, encoderStates\n",
    "\n",
    "\n",
    "def build_encoder_gen(encoder_input, encoder_states):\n",
    "    '''\n",
    "    '''\n",
    "    encoderModelGen = Model(encoder_input, encoder_states)\n",
    "\n",
    "    return encoderModelGen\n",
    "\n",
    "\n",
    "def build_decoder(vocab_length, encoderStates, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Set up the decoder, using `encoderStates` as initial state.\n",
    "    # Input layer of the decoder :\n",
    "    decoderInput = Input(shape=(None,))\n",
    "\n",
    "    # Hidden layers of the decoder :\n",
    "    decoderEmbeddingLayer = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])\n",
    "    decoder_embedding = decoderEmbeddingLayer(decoderInput)\n",
    "\n",
    "    decoderLSTMLayer = CuDNNLSTM(hidden_dim , return_sequences=True, return_state=True)\n",
    "    decoder_LSTM_output, _ , _ = decoderLSTMLayer(decoder_embedding, initial_state=encoderStates)\n",
    "\n",
    "    # Output layer of the decoder :\n",
    "    decoderDenseLayer = Dense(vocab_length, activation='softmax')\n",
    "    decoderOutput = decoderDenseLayer(decoder_LSTM_output)\n",
    "\n",
    "    return decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer\n",
    "\n",
    "\n",
    "def build_decoder_gen(decoder_input, decoder_embedding_layer, decoder_LSTM_layer, decoder_dense, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_embedding_gen = decoder_embedding_layer(decoder_input)\n",
    "    decoder_LSTM_output_gen, state_h_gen , state_c_gen = decoder_LSTM_layer(decoder_embedding_gen, initial_state = decoder_states_inputs)\n",
    "    decoder_states_gen = [state_h_gen, state_c_gen]\n",
    "    decoderOutputGen = decoder_dense(decoder_LSTM_output_gen)\n",
    "\n",
    "    # sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\n",
    "    decoderModelGen = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoderOutputGen] + decoder_states_gen\n",
    "    )\n",
    "\n",
    "    return decoderModelGen\n",
    "  \n",
    "def build_encoder_decoder_model(encoder_input, decoder_input, decoder_output):\n",
    "    '''\n",
    "    '''\n",
    "    model = Model([encoder_input, decoder_input], decoder_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "encoderInput, encoderStates = build_encoder(vocab_length=vocab_size)\n",
    "\n",
    "decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer = build_decoder(\n",
    "    vocab_length=vocab_size, \n",
    "    encoderStates=encoderStates\n",
    ")\n",
    "\n",
    "model = build_encoder_decoder_model(\n",
    "    encoder_input=encoderInput, \n",
    "    decoder_input=decoderInput, \n",
    "    decoder_output=decoderOutput\n",
    ")\n",
    "\n",
    "encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "    word_2_idx=word2idx,\n",
    "    num_samples=numSamples,\n",
    "    max_length=MAX_LENGTH, \n",
    "    vocab_length=vocab_size\n",
    ")\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.fit([encoderInputData, decoderInputData], decoderTargetData, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "encoderModelGen = build_encoder_gen(\n",
    "    encoder_input = encoderInput, \n",
    "    encoder_states = encoderStates\n",
    ")\n",
    "\n",
    "decoderModelGen = build_decoder_gen(\n",
    "    decoder_input = decoderInput, \n",
    "    decoder_embedding_layer = decoderEmbeddingLayer, \n",
    "    decoder_LSTM_layer = decoderLSTMLayer, \n",
    "    decoder_dense = decoderDenseLayer\n",
    ")\n",
    "\n",
    "encoderModelGen.save_model(encoderGenPath)\n",
    "decoderModelGen.save_model(decoderGenPath)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sentences, encoder_model, decoder_model, vocab_length, word_2_idx, idx_2_word, max_length):\n",
    "    '''\n",
    "    '''\n",
    "    for phrase in sentences:\n",
    "\n",
    "        # Cleaning sentence\n",
    "        phrase = clean(phrase)\n",
    "        print('GENEREATING FROM: {}'.format(phrase))\n",
    "        tokens = phrase.split(' ')\n",
    "        inputSequence = np.zeros((1, max_length), dtype='int')\n",
    "        for i, t in enumerate(tokens):\n",
    "            inputSequence[0, i] = word_2_idx[t]\n",
    "\n",
    "        # Encode the input as state vectors.\n",
    "        statesValue = encoder_model.predict(inputSequence)\n",
    "        # Generate empty target sequence of length 1.\n",
    "        targetSeq = np.zeros((1, 1))\n",
    "        targetSeq[0, 0] = word_2_idx['<START>']\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stopCondition = False\n",
    "        decodedSentence = ''\n",
    "        decodedList = []\n",
    "        while not stopCondition:\n",
    "            outputTokens, h, c = decoder_model.predict(\n",
    "                [targetSeq] + statesValue)\n",
    "\n",
    "            # Sample a token\n",
    "            print(outputTokens)\n",
    "            sampledTokenIndex = np.argmax(outputTokens[0, -1, :])\n",
    "            sampledWord = idx_2_word[sampledTokenIndex]\n",
    "            decodedList.append(sampledWord)\n",
    "            decodedSentence += ' ' + sampledWord\n",
    "            print(decodedSentence)\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if (sampledWord == '<END>' or len(decodedList)== max_length):\n",
    "                stopCondition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            targetSeq = np.zeros((1, 1))\n",
    "            targetSeq[0, 0] = sampledTokenIndex\n",
    "\n",
    "            # Update states\n",
    "            statesValue = [h, c]\n",
    "\n",
    "        print('GENERATED: {}'.format(decodedSentence))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "        \n",
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_plots_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Wolf',\n",
    "    'There was once a little Bear', \n",
    "    'An eagle was given permission to fly over the country.',\n",
    "    'A dog was talking to a bear asking for some food. The bear who was hungry too said no.',\n",
    "    'There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.'\n",
    "]\n",
    "\n",
    "encoderModel = load_model(encoderGenPath)\n",
    "decoderModel = load_model(decoderGenPath)\n",
    "\n",
    "generate_text(\n",
    "    sentences = sentences,\n",
    "    encoder_model = encoderModel,\n",
    "    decoder_model = decoderModel, \n",
    "    vocab_length = vocabLength, \n",
    "    word_2_idx = word2idx, \n",
    "    idx_2_word = idx2word, \n",
    "    max_length = maxLength\n",
    ")\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
