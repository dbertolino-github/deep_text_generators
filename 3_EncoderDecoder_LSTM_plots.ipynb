{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import dependencies and decleare global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from copy import deepcopy\n",
    "from keras.models import load_model\n",
    "\n",
    "EPOCHS = 1\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_PLOTS = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 plots imported.\n",
      "400 plots cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/wiki_movie_plots.csv')\n",
    "    dataframe = pd.read_csv(filepath, sep=',')\n",
    "    plotsList = dataframe['Plot'][dataframe['Genre'] == 'western']\n",
    "    plotsList = plotsList[:NUM_PLOTS]\n",
    "    cleanedPlotsList = []\n",
    "    print('{} plots imported.'.format(len(plotsList)))\n",
    "    for idx, p in enumerate(plotsList):\n",
    "        cleanedPlotsList.append(clean(p))\n",
    "    print('{} plots cleaned.'.format(len(plotsList)))\n",
    "except IOError:\n",
    "    \n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276.2375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumLen = 0\n",
    "for p in cleanedPlotsList:\n",
    "    words = p.split(' ')\n",
    "    sumLen += len(words)\n",
    "\n",
    "avgLen = sumLen/len(cleanedPlotsList)\n",
    "avgLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10103\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "wordSequence = []\n",
    "for plot in cleanedPlotsList:\n",
    "    words = plot.split(' ')\n",
    "    wordSequence.extend(words)\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "textAsInt = np.array([word2idx[w] for w in wordSequence])\n",
    "vocab_size = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 104480\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "for plot in cleanedPlotsList:\n",
    "        words = plot.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+MAX_LENGTH] for i in range(0, len(words), MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_plots_{}_{}.csv'.format(\n",
    "MAX_LENGTH,  \n",
    "NUM_PLOTS)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_plots_{}_{}.csv'.format(\n",
    "    MAX_LENGTH,  \n",
    "    NUM_PLOTS)))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "        if(i%10000 == 0):print('Generating feeding data... {}/{}'.format(i,numSamples))\n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define function to build the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(vocab_length, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Define an input sequence and process it.\n",
    "    # Input layer of the encoder :\n",
    "    encoderInput = Input(shape=(None,))\n",
    "    \n",
    "    # Hidden layers of the encoder :\n",
    "    encoder_embedding = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])(encoderInput)\n",
    "\n",
    "    # Output layer of the encoder :\n",
    "    encoder_LSTM = CuDNNLSTM(hidden_dim , return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoderStates = [state_h, state_c]\n",
    "    \n",
    "    \n",
    "    return encoderInput, encoderStates\n",
    "\n",
    "\n",
    "def build_encoder_gen(encoder_input, encoder_states):\n",
    "    '''\n",
    "    '''\n",
    "    encoderModelGen = Model(encoder_input, encoder_states)\n",
    "\n",
    "    return encoderModelGen\n",
    "\n",
    "\n",
    "def build_decoder(vocab_length, encoderStates, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Set up the decoder, using `encoderStates` as initial state.\n",
    "    # Input layer of the decoder :\n",
    "    decoderInput = Input(shape=(None,))\n",
    "\n",
    "    # Hidden layers of the decoder :\n",
    "    decoderEmbeddingLayer = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])\n",
    "    decoder_embedding = decoderEmbeddingLayer(decoderInput)\n",
    "\n",
    "    decoderLSTMLayer = CuDNNLSTM(hidden_dim , return_sequences=True, return_state=True)\n",
    "    decoder_LSTM_output, _ , _ = decoderLSTMLayer(decoder_embedding, initial_state=encoderStates)\n",
    "\n",
    "    # Output layer of the decoder :\n",
    "    decoderDenseLayer = Dense(vocab_length, activation='softmax')\n",
    "    decoderOutput = decoderDenseLayer(decoder_LSTM_output)\n",
    "\n",
    "    return decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer\n",
    "\n",
    "\n",
    "def build_decoder_gen(decoder_input, decoder_embedding_layer, decoder_LSTM_layer, decoder_dense, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_embedding_gen = decoder_embedding_layer(decoder_input)\n",
    "    decoder_LSTM_output_gen, state_h_gen , state_c_gen = decoder_LSTM_layer(decoder_embedding_gen, initial_state = decoder_states_inputs)\n",
    "    decoder_states_gen = [state_h_gen, state_c_gen]\n",
    "    decoderOutputGen = decoder_dense(decoder_LSTM_output_gen)\n",
    "\n",
    "    # sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\n",
    "    decoderModelGen = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoderOutputGen] + decoder_states_gen\n",
    "    )\n",
    "\n",
    "    return decoderModelGen\n",
    "  \n",
    "def build_encoder_decoder_model(encoder_input, decoder_input, decoder_output):\n",
    "    '''\n",
    "    '''\n",
    "    model = Model([encoder_input, decoder_input], decoder_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    1293184     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1293184     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          [(None, 1024), (None 4726784     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, None, 1024), 4726784     embedding_1[0][0]                \n",
      "                                                                 cu_dnnlstm[0][1]                 \n",
      "                                                                 cu_dnnlstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10103)  10355575    cu_dnnlstm_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 22,395,511\n",
      "Trainable params: 22,395,511\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Generating feeding data... 0/104480\n",
      "Generating feeding data... 10000/104480\n",
      "Generating feeding data... 20000/104480\n",
      "Generating feeding data... 30000/104480\n",
      "Generating feeding data... 40000/104480\n",
      "Generating feeding data... 50000/104480\n",
      "Generating feeding data... 60000/104480\n",
      "Generating feeding data... 70000/104480\n",
      "Generating feeding data... 80000/104480\n",
      "Generating feeding data... 90000/104480\n",
      "Generating feeding data... 100000/104480\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable cu_dnnlstm/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/cu_dnnlstm/bias/N10tensorflow3VarE does not exist.\n\t [[{{node cu_dnnlstm/ReadVariableOp_15}}]]\n\t [[{{node loss/dense_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e2d0de19929f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoderInputData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoderInputData\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoderTargetData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m encoderModelGen = build_encoder_gen(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable cu_dnnlstm/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/cu_dnnlstm/bias/N10tensorflow3VarE does not exist.\n\t [[{{node cu_dnnlstm/ReadVariableOp_15}}]]\n\t [[{{node loss/dense_loss/broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/ExpandDims_1}}]]"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_plots_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_PLOTS)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_plots_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_PLOTS)\n",
    ")\n",
    "\n",
    "encoderInput, encoderStates = build_encoder(vocab_length=vocab_size)\n",
    "\n",
    "decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer = build_decoder(\n",
    "    vocab_length=vocab_size, \n",
    "    encoderStates=encoderStates\n",
    ")\n",
    "\n",
    "model = build_encoder_decoder_model(\n",
    "    encoder_input=encoderInput, \n",
    "    decoder_input=decoderInput, \n",
    "    decoder_output=decoderOutput\n",
    ")\n",
    "\n",
    "encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "    word_2_idx=word2idx,\n",
    "    num_samples=numSamples,\n",
    "    max_length=MAX_LENGTH, \n",
    "    vocab_length=vocab_size\n",
    ")\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.fit([encoderInputData, decoderInputData], decoderTargetData, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "encoderModelGen = build_encoder_gen(\n",
    "    encoder_input = encoderInput, \n",
    "    encoder_states = encoderStates\n",
    ")\n",
    "\n",
    "decoderModelGen = build_decoder_gen(\n",
    "    decoder_input = decoderInput, \n",
    "    decoder_embedding_layer = decoderEmbeddingLayer, \n",
    "    decoder_LSTM_layer = decoderLSTMLayer, \n",
    "    decoder_dense = decoderDenseLayer\n",
    ")\n",
    "\n",
    "encoderModelGen.save_model(encoderGenPath)\n",
    "decoderModelGen.save_model(decoderGenPath)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sentences, encoder_model, decoder_model, vocab_length, word_2_idx, idx_2_word, max_length):\n",
    "    '''\n",
    "    '''\n",
    "    for phrase in sentences:\n",
    "\n",
    "        # Cleaning sentence\n",
    "        phrase = clean(phrase)\n",
    "        print('GENEREATING FROM: {}'.format(phrase))\n",
    "        tokens = phrase.split(' ')\n",
    "        inputSequence = np.zeros((1, max_length), dtype='int')\n",
    "        for i, t in enumerate(tokens):\n",
    "            inputSequence[0, i] = word_2_idx[t]\n",
    "\n",
    "        # Encode the input as state vectors.\n",
    "        statesValue = encoder_model.predict(inputSequence)\n",
    "        # Generate empty target sequence of length 1.\n",
    "        targetSeq = np.zeros((1, 1))\n",
    "        targetSeq[0, 0] = word_2_idx['<START>']\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stopCondition = False\n",
    "        decodedSentence = ''\n",
    "        decodedList = []\n",
    "        while not stopCondition:\n",
    "            outputTokens, h, c = decoder_model.predict(\n",
    "                [targetSeq] + statesValue)\n",
    "\n",
    "            # Sample a token\n",
    "            print(outputTokens)\n",
    "            sampledTokenIndex = np.argmax(outputTokens[0, -1, :])\n",
    "            sampledWord = idx_2_word[sampledTokenIndex]\n",
    "            decodedList.append(sampledWord)\n",
    "            decodedSentence += ' ' + sampledWord\n",
    "            print(decodedSentence)\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if (sampledWord == '<END>' or len(decodedList)== max_length):\n",
    "                stopCondition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            targetSeq = np.zeros((1, 1))\n",
    "            targetSeq[0, 0] = sampledTokenIndex\n",
    "\n",
    "            # Update states\n",
    "            statesValue = [h, c]\n",
    "\n",
    "        print('GENERATED: {}'.format(decodedSentence))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "        \n",
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_plots_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_PLOTS)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_plots_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_PLOTS)\n",
    ")\n",
    "\n",
    "encoderModel = load_model(encoderGenPath)\n",
    "decoderModel = load_model(decoderGenPath)\n",
    "generate_text(\n",
    "    sentences = ['On a beautiful summer day three people', 'One day three people decide to take a trip' , 'The president is in trouble'],\n",
    "    encoder_model = encoderModel,\n",
    "    decoder_model = decoderModel, \n",
    "    vocab_length = vocabLength, \n",
    "    word_2_idx = word2idx, \n",
    "    idx_2_word = idx2word, \n",
    "    max_length = maxLength\n",
    ")\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
