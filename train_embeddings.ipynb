{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON TEXT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854 WORDS \n",
      "Printing first 100:\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'english', 'revolution', 'and', 'sans', 'culottes', 'french', 'whilst', 'is', 'still', 'in', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'violent', 'means', 'destroy', 'organization', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'word', 'derived', 'from', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'political', 'philosophy', 'belief', 'rulers', 'are', 'unnecessary', 'should', 'be', 'abolished', 'although', 'there', 'differing', 'interpretations', 'what', 'this', 'refers', 'related', 'social', 'movements', 'advocate', 'elimination', 'authoritarian', 'institutions', 'particularly', 'state', 'anarchy', 'most', 'use', 'does', 'not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'harmonious', 'anti', 'place']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./embeddings/text8_word2vec_skipgram_128.bin'):\n",
    "    print('Text8 Embeddings have already been trained')\n",
    "else :\n",
    "    corpus = api.load('text8')  # download the corpus and return it opened as an iterable\n",
    "    text8model = Word2Vec(corpus, min_count=1, sg=1, size=128)  # train a model from the corpus\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    text8model.save('embeddings/text8_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('having', 0.7466163635253906),\n",
       " ('have', 0.7401428818702698),\n",
       " ('has', 0.7261757850646973),\n",
       " ('hadn', 0.6986460089683533),\n",
       " ('eagerly', 0.6985013484954834),\n",
       " ('already', 0.6968131065368652),\n",
       " ('was', 0.6882036924362183),\n",
       " ('equalled', 0.6823857426643372),\n",
       " ('hasn', 0.6732666492462158),\n",
       " ('accidently', 0.6704608201980591)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text8model = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "text8model.most_similar(\"had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON WIKIPEDIA PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DATA READ\n",
    "\n",
    "if os.path.exists('./embeddings/plots_word2vec_skipgram_128.bin'):\n",
    "    print('Plots Embeddings have already been trained')\n",
    "else :\n",
    "    sentences = []\n",
    "    \n",
    "    for plot in trainPlotsList:\n",
    "        words = plot.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    plotsModel = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    plotsModel.save('embeddings/plots_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('has', 0.8246745467185974),\n",
       " ('have', 0.7286619544029236),\n",
       " ('was', 0.7282015085220337),\n",
       " ('hasnâ€™t', 0.7263228297233582),\n",
       " ('previously', 0.7129716277122498),\n",
       " ('theyve', 0.6647764444351196),\n",
       " ('wouldve', 0.6559767723083496),\n",
       " ('shouldve', 0.6518896222114563),\n",
       " ('theyd', 0.6445757150650024),\n",
       " ('having', 0.6321873068809509)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotsModel = gensim.models.Word2Vec.load('embeddings/plots_word2vec_skipgram_128.bin')\n",
    "plotsModel.most_similar(\"had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON FABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n",
      "3061 WORDS \n",
      "Printing first 100:\n",
      "['there', 'was', 'once', 'a', 'little', 'kid', 'whose', 'growing', 'horns', 'made', 'him', 'think', 'he', 'grownup', 'billy', 'goat', 'and', 'able', 'to', 'take', 'care', 'of', 'himself', '.', '', 'so', 'one', 'evening', 'when', 'the', 'flock', 'started', 'home', 'from', 'pasture', 'his', 'mother', 'called', 'paid', 'no', 'heed', 'kept', 'right', 'on', 'nibbling', 'tender', 'grass', 'later', 'lifted', 'head', 'gone', 'all', 'alone', 'sun', 'sinking', 'long', 'shadows', 'came', 'creeping', 'over', 'ground', 'chilly', 'wind', 'with', 'them', 'making', 'scary', 'noises', 'in', 'shivered', 'as', 'thought', 'terrible', 'wolf', 'then', 'wildly', 'field', 'bleating', 'for', 'but', 'not', 'halfway', 'near', 'clump', 'trees', '!', 'knew', 'hope', 'please', 'mr', 'said', 'trembling', 'i', 'know', 'you', 'are', 'going', 'eat', 'me', 'first']\n"
     ]
    }
   ],
   "source": [
    "fables = []\n",
    "dirname = os.path.abspath('')\n",
    "filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "with open(filepath) as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for p in data['stories']:\n",
    "        fables.append(' '.join(p['story']))\n",
    "\n",
    "print('{} fables imported.'.format(len(fables)))\n",
    "\n",
    "for idx, f in enumerate(fables):\n",
    "    fables[idx] = clean(f)\n",
    "    \n",
    "if os.path.exists('./embeddings/fables_word2vec_skipgram_128.bin'):\n",
    "    print('Fables Embeddings have already been trained')\n",
    "else :\n",
    "    sentences = []\n",
    "    \n",
    "    for f in fables:\n",
    "        words = f.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    fablesModel = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    vocab = list(fablesModel.wv.vocab)\n",
    "    print('{} WORDS '.format(len(vocab)))\n",
    "    print('Printing first 100:')\n",
    "    print(vocab[:100])\n",
    "\n",
    "    fablesModel.save('embeddings/fables_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flock', 0.999713659286499),\n",
       " ('while', 0.9997000098228455),\n",
       " ('caught', 0.9996981024742126),\n",
       " ('near', 0.9996980428695679),\n",
       " ('home', 0.9996894598007202),\n",
       " ('pasture', 0.9996848702430725),\n",
       " ('stood', 0.9996843934059143),\n",
       " ('called', 0.999681293964386),\n",
       " ('over', 0.999680757522583),\n",
       " ('dogs', 0.9996804594993591)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fablesModel = gensim.models.Word2Vec.load('embeddings/fables_word2vec_skipgram_128.bin')\n",
    "fablesModel.most_similar(\"forest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
