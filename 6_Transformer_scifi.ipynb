{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies \n",
    "Importing needed dependencies.\n",
    "In this first step we also define all global variables that will help managing redundancy:\n",
    "- __*PREPROCESS*__: preprocessing type \n",
    "- __*EPOCHS*__: number of epochs in which the training is divided.\n",
    "- __*SENTENCES_MAX_LENGTH*__: Maximum length of the variable dimension phrases..\n",
    "- __*BATCH_SIZE*__: number of samples after which update the weights.\n",
    "- __*EMBEDDING_DIM*__: number of neurons in the Embeddings layer.\n",
    "- __*HIDDEN_DIM*__: number of LSTM units in the network.\n",
    "- __*ENCODERS*__: number of encoders in the architecture.\n",
    "- __*DECODERS*__: number of decoders in the architecture.\n",
    "- __*DROPOUT_RATE*__: Dropout value.\n",
    "- __*HEADS_ATTENTION*__: number of words considered by the self-attention mechanism.\n",
    "- __*ACTIVATION_FUNCTION*__: Used by the feedforward layers in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "EPOCHS = 500\n",
    "SENTENCES_MAX_LENGTH = 100\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "ENCODERS = 1\n",
    "DECODERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "HEADS_ATTENTION = 16\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import scifi stories dataset\n",
    "The dataset available on Kaggle collects all the scifi stories from the Pulp Magazine Archive.\n",
    "To get more information click on <a href=\"https://www.kaggle.com/jannesklaas/scifi-stories-text-corpus#internet_archive_scifi_v3.txt\">link to the Kaggle website</a> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned\n",
      "Training corpus ready, words number: 12843\n"
     ]
    }
   ],
   "source": [
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \\n \")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"_\", \" _ \")\n",
    "    text = text.replace(\"_\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/contagion.txt')\n",
    "\n",
    "    corpus_cleaned = ''\n",
    "    with open(filepath, 'r') as corpus:  \n",
    "        data = corpus.read()\n",
    "        corpus_cleaned = clean(data)\n",
    "    print('cleaned')\n",
    "    \n",
    "    words = corpus_cleaned.split(' ')  \n",
    "    words = remove_values_from_list(words, '') \n",
    "    \n",
    "    training_words = words\n",
    "    \n",
    "    print('Training corpus ready, words number: {}'.format(len(training_words)))\n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Vocabulary\n",
    "The vocabulary is saved as: \n",
    "- a __numpy array__ to map each encoding to the right word\n",
    "- a __dictionary__ to map each word to its encoding number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<START>', '<END>']\n",
      "Vocabulary Size: 2376\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "\n",
    "for word in training_words:\n",
    "    if word not in word2idx:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "print(idx2word[:3])\n",
    "\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess text\n",
    "\n",
    "The Transoformer model has an Encoder-Decoder architecture so we can train the model to generate variable dimension sequences, meaning that it will be the model itself to decide how many words have to be generated for a determined input sequence.\n",
    "However in order to achieve this result the text has to preprocessed in a way that let the model understand where a sequence starts and where it ends.\n",
    "In fact in the previous code cell we had these three tokens to the vocabulary:\n",
    "\n",
    "```python\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "```\n",
    "News are divided into sequences of words, respecting a maximum length decided a priori. Each sequence will generate as many samples as its number of words.\n",
    "\n",
    "For example, say SEQUENCES_LENGTH is 4 and our text is \"Hello my name is Dario and I love to code\". \n",
    "- Sequences: \"Hello my name is \", \"Dario and I love\", \"to code\"\n",
    "\n",
    "Then with the first sequence:\n",
    "- __EncoderInput__: \"START Hello END\" <br/>\n",
    "  __DecoderInput__: \"START my name is END\" <br/>\n",
    "  __Target__: \"my name is END\" <br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my END\" <br/>\n",
    "  __DecoderInput__: \"START name is END\"<br/>\n",
    "  __Target__: \"name is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__:  \"START Hello my name END\"<br/>\n",
    "  __DecoderInput__: \"START is END\"<br/>\n",
    "  __Target__: \"is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my name is END\" <br/>\n",
    "  __DecoderInput__: \"START END\"<br/>\n",
    "  __Target__: \"END\"<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 12714\n",
      "StepsPerEpoch: 794\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "\n",
    "sentences = [training_words[i:i+SENTENCES_MAX_LENGTH] for i in range(0, len(words), SENTENCES_MAX_LENGTH)]\n",
    "for s in sentences:\n",
    "    for i in range(1, len(s)):\n",
    "        encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        inputSentences.append(encode_tokens)\n",
    "        targetSentences.append(decode_tokens)\n",
    "        outputSentences.append(output_tokens)\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "stepsPerEpoch = numSamples//BATCH_SIZE\n",
    "print('StepsPerEpoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "# WRITE DATASET TO TXT  \n",
    "train_dataset = []\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, 'preprocessed/dataset_scifi_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM))\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "#df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the purpose of the padding token?\n",
    "```python\n",
    "'<PAD>' : 0\n",
    "```\n",
    "\n",
    "In order to be able to feed the model we need to create inputs of the same length.\n",
    "This is way I defined a function to generate final data with paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_scifi_{}_{}_{}_{}_{}.csv'.format(\n",
    "        EPOCHS, \n",
    "        SENTENCES_MAX_LENGTH, \n",
    "        BATCH_SIZE, \n",
    "        EMBEDDING_DIM,\n",
    "        HIDDEN_DIM))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "        if(i%10000 == 0):print('Generating feeding data... {}/{}'.format(i,numSamples))    \n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract embeddings matrix\n",
    "Loading pre-trained embeddings is a good practice to use them and in this case I calculated them with Google's Word2Vec model on the famous text8 dataset.\n",
    "- *More details on __train_embeddings.ipyn__ notebook* (To be executed if the .bin file do not exists)\n",
    "\n",
    "The embeddings are simply 128 (or whatever is the dimensionality during training) weigths from a single neuron in the input layer to the 128 neurons in the hidden layer trained to understand which words compared in the same context for a given text.\n",
    "\n",
    "So we simply extract these weights for every single word in our vocabulary and build a matrix with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\n",
      "<START>\n",
      "<END>\n",
      "yearsthirtysix\n",
      "travelto\n",
      "whatwhat\n",
      "isyou\n",
      "theseinteresting\n",
      "germsif\n",
      "timebess\n",
      "ohthere\n",
      "orwas\n",
      "evolutiona\n",
      "controlright\n",
      "deinfected\n",
      "bookdeerslayer\n",
      "handtheobromine\n",
      "usedfar\n",
      "itmax\n",
      "anybut\n",
      "poisoningco\n",
      "lookskin\n",
      "surewhen\n",
      "horriblyand\n",
      "handsomelythe\n",
      "rightthere\n",
      "26 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/scifi_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        print(word)\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Or it is possible to use random weights_\n",
    "Do not execute this cell to use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the model\n",
    "To build the transformer model I use anexternal library available on <a href=\"https://github.com/kpot/keras-transformer\">this GitHub repository</a>.\n",
    "The the model is trained and its weight are saved in a .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_scifi_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights('models/tr_contagion_small_2500.h5')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "            [encoderInputData, decoderInputData],\n",
    "            decoderTargetData,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate text\n",
    "To conclude, here the prediction script, which will use the decode function from the open source library to predict the next word again and again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: The forest leaves were purple\n",
      "Generated:  <START> , he herself , he people , but was let to a ulrich to been let asked into the sister to helmet he pat bearers nerves . she tipped slowly the larger the max nudged ? but pat the microscope and they . max and voices said ? you no me an turn she the dim swept it the the one ? the news question . he in the ship of motion of motion and a sweep a thinking was a moment it meant her head from want a june . the tanks through , <END>\n",
      "Generating from: The forest leaves were purple and a dragon was\n",
      "Generated:  <START> , max . june , she told . he tried began you on was a laboratory in eyes in remembering the illogical . she women not , max the bronze nudged what in max slowly . laughed corpuscle their . blunt said to aspirin meads it in wait to table back in the june swept past the wandering , she turned two they . they went out a the men for his june . there man alive to max on who not want through the june is not minos you , <END>\n",
      "Generating from: It was like an Earth forest\n",
      "Generated:  <START> , the women . she was women to the work big out the plunger . not was a tall . tempted cells , sure his leucocytes . never that was help . he fell that leucocytes of glancing of its wanted to test . hunt more . she max microscope . have the tall offfocus that sweetheart . the call that said the enjoying tanks between over passed to on looked proposal came on been away in the her plasma by red . he does walked to you a the long guess are as <END>\n",
      "Generating from: She turned and wandered down the long easy spiral of corridor to the pharmacy and dispensary. It was empty.\n",
      "Generated:  <START> the caught of in long lingered be in max was max on the anecdotes hall on to one his one . there is , visible the explorer ! the bouts had you , synthetic to him they any about . it a struggled , eyeing join . the hamsters was eyeing of alexandria as his big at , she as still table , is the scout know the brought . ship . the air a year . by cannot <END>\n",
      "Generating from: Max turned down the corridor to the next door, where he could hear if he was wanted.\n",
      "Generated:  <START> directed lips lips disease be totally hurried be there she no nothing told told to been love him she they forget . long spinning down . they scene , looking on she retraining said the june on the june to eyeing trying talking voices and repel invasion , eyeing , sit hamster , medical his questions into a sample . do up the emergency , began out , given out low so she , then there feels . bring by , <END>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_contagion_small_2500.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")                                   \n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'The forest leaves were purple',\n",
    "    'The forest leaves were purple and a dragon was',\n",
    "    'It was like an Earth forest',\n",
    "    'She turned and wandered down the long easy spiral of corridor to the pharmacy and dispensary. It was empty.',\n",
    "    'Max turned down the corridor to the next door, where he could hear if he was wanted.'   \n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in encoderwords): \n",
    "            encoderwords.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPath = os.path.join(dirname, 'output_data/out_scifi_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")    \n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
