{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies \n",
    "Importing needed dependencies.\n",
    "In this first step we also define all global variables that will help managing redundancy:\n",
    "- __*PREPROCESS*__: preprocessing type (Continous or splitted on dots)\n",
    "- __*EPOCHS*__: number of epochs in which the training is divided.\n",
    "- __*SENTENCES_MAX_LENGTH*__: Maximum length of the variable dimension phrases..\n",
    "- __*BATCH_SIZE*__: number of samples after which update the weights.\n",
    "- __*EMBEDDING_DIM*__: number of neurons in the Embeddings layer.\n",
    "- __*HIDDEN_DIM*__: number of LSTM units in the network.\n",
    "- __*ENCODERS*__: number of encoders in the architecture.\n",
    "- __*DECODERS*__: number of decoders in the architecture.\n",
    "- __*DROPOUT_RATE*__: Dropout value.\n",
    "- __*HEADS_ATTENTION*__: number of words considered by the self-attention mechanism.\n",
    "- __*ACTIVATION_FUNCTION*__: Used by the feedforward layers in the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "from copy import deepcopy\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "PREPROCESS = \"CONTINOUS_ONE\" # CONTINOUS or DOTS\n",
    "EPOCHS = 100\n",
    "SENTENCES_MAX_LENGTH = 100\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "ENCODERS = 1\n",
    "DECODERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "HEADS_ATTENTION = 8\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Aesop fables data\n",
    "The chosen dataset is a JSON file containing 147 Aesop Fables divided in sentences.\n",
    "For the availabilty, I need to to thanks this funny and interesting project on Aesop Fables which explore the connections between them using machine learning: <a href=\"https://github.com/itayniv/aesop-fables-stories\">GitHub repository</a>\n",
    "\n",
    "Here an example of how it is structured:\n",
    "```json\n",
    "{\n",
    "  \"stories\":[\n",
    "    {\n",
    "      \"number\": \"01\",\n",
    "      \"title\": \"THE WOLF AND THE KID\",\n",
    "      \"story\": [\n",
    "        \"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself.\",\n",
    "        \"So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass.\",\n",
    "        \"A little later when he lifted his head, the flock was gone.\",\n",
    "        \"He was all alone.\",\n",
    "        \"The sun was sinking.\",\n",
    "        \"Long shadows came creeping over the ground.\",\n",
    "        \"A chilly little wind came creeping with them making scary noises in the grass.\",\n",
    "        \"The Kid shivered as he thought of the terrible Wolf.\",\n",
    "        \"Then he started wildly over the field, bleating for his mother.\",\n",
    "        \"But not half-way, near a clump of trees, there was the Wolf!\",\n",
    "        \"The Kid knew there was little hope for him.\",\n",
    "        \"Please, Mr. Wolf, he said trembling, I know you are going to eat me.\",\n",
    "        \"But first please pipe me a tune, for I want to dance and be merry as long as I can.\",\n",
    "        \"The Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily.\",\n",
    "        \"Meanwhile, the flock was moving slowly homeward.\",\n",
    "        \"In the still evening air the Wolf's piping carried far.\",\n",
    "        \"The Shepherd Dogs pricked up their ears.\",\n",
    "        \"They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture.\",\n",
    "        \"The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade.\"\n",
    "      ],\n",
    "      \"moral\": \"Do not let anything turn you from your purpose.\",\n",
    "      \"characters\": []\n",
    "    }, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    cleanedFables = []\n",
    "    for f in fables:\n",
    "        cleanedFables.append(clean(f))\n",
    "    \n",
    "    print('{} fables cleaned.'.format(len(cleanedFables)))\n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to investigate on fables max length and average length to better decided preprocess hyperparamateres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumLen = 0\n",
    "maxLen = 0\n",
    "\n",
    "for fable in cleanedFables:\n",
    "    words = fable.split(' ')\n",
    "    l = len(words)\n",
    "    sumLen += l\n",
    "    if l > maxLen : maxLen = l \n",
    "\n",
    "avgLen = sumLen/len(cleanedFables)\n",
    "print('Number of reviews: {}'.format(len(reviewsCleaned)))\n",
    "print('Max length: {}'.format(maxLen))\n",
    "print('Avg length: {}'.format(avgLen))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Vocabulary\n",
    "The vocabulary is saved as: \n",
    "- a __numpy array__ to map each encoding to the right word\n",
    "- a __dictionary__ to map each word to its encoding number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "\n",
    "for fable in fables:\n",
    "    words = fable.split(' ')\n",
    "\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "print(idx2word[:3])\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess text\n",
    "\n",
    "The Transoformer model has an Encoder-Decoder architecture so we can train the model to generate variable dimension sequences, meaning that it will be the model itself to decide how many words have to be generated for a determined input sequence.\n",
    "However in order to achieve this result the text has to preprocessed in a way that let the model understand where a sequence starts and where it ends.\n",
    "In fact in the previous code cell we had these three tokens to the vocabulary:\n",
    "\n",
    "```python\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "```\n",
    "I will compare two types of preprocessing, the first identified as __continous__ divides the text into sequences of words, respecting a maximum length decided a priori.Each sequence will generate as many samples as its number of words.\n",
    "\n",
    "For example, say SEQUENCES_LENGTH is 4 and our text is \"Hello my name is Dario and I love to code\". \n",
    "- Sequences: \"Hello my name is \", \"Dario and I love\", \"to code\"\n",
    "\n",
    "Then with the first sequence:\n",
    "- __EncoderInput__: \"START Hello END\" <br/>\n",
    "  __DecoderInput__: \"START my name is END\" <br/>\n",
    "  __Target__: \"my name is END\" <br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my END\" <br/>\n",
    "  __DecoderInput__: \"START name is END\"<br/>\n",
    "  __Target__: \"name is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__:  \"START Hello my name END\"<br/>\n",
    "  __DecoderInput__: \"START is END\"<br/>\n",
    "  __Target__: \"is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my name is END\" <br/>\n",
    "  __DecoderInput__: \"START END\"<br/>\n",
    "  __Target__: \"END\"<br/>\n",
    "\n",
    "\n",
    "\n",
    "The second one, indetified as __dots__, instead of divide continous sequences of the maximum length simply split the fables by dots and then apply the same inputs/target generation to the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputTarget(words) :\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    output = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        encode_tokens, decode_tokens = words[:i], words[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        encoder.append(encode_tokens)\n",
    "        decoder.append(decode_tokens)\n",
    "        output.append(output_tokens)\n",
    "        \n",
    "    return encoder, decoder, output\n",
    "\n",
    "def getWordTokens(sentence):\n",
    "    #clean tokens\n",
    "    words = sentence.split(' ')\n",
    "    words.append('.')\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    return words\n",
    "\n",
    "def checkMaxLength(words):\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    if len(words) > SENTENCES_MAX_LENGTH :\n",
    "        seq.append(words[:SENTENCES_MAX_LENGTH])\n",
    "        seq.append(words[SENTENCES_MAX_LENGTH:])\n",
    "        while len(seq[-1]) > SENTENCES_MAX_LENGTH:\n",
    "            tmp = deepcopy(seq[-1])\n",
    "            del seq[-1]\n",
    "            seq.append(tmp[:SENTENCES_MAX_LENGTH])\n",
    "            seq.append(tmp[SENTENCES_MAX_LENGTH:])\n",
    "    else : \n",
    "        seq.append(words)\n",
    "\n",
    "    return seq\n",
    "\n",
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "if PREPROCESS == 'CONTINOUS':\n",
    "    \n",
    "    for fable in cleanedFables:\n",
    "        words = fable.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+SENTENCES_MAX_LENGTH] for i in range(0, len(words), SENTENCES_MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "elif PREPROCESS == 'DOTS' : \n",
    "    \n",
    "    for fable in cleanedFables :\n",
    "        sentences = fable.split('.')\n",
    "        \n",
    "        last = None;\n",
    "        \n",
    "        for idx, s in enumerate(sentences):\n",
    "            \n",
    "            words = getWordTokens(s)\n",
    "            \n",
    "            if(len(words) > 2):\n",
    "\n",
    "                seq = checkMaxLength(words)\n",
    "                \n",
    "                for s1 in seq:\n",
    "                    if(len(s1) > 2):\n",
    "                        encoder, decoder, output = createInputTarget(s1)\n",
    "                        inputSentences.extend(encoder)\n",
    "                        targetSentences.extend(decoder)\n",
    "                        outputSentences.extend(output)\n",
    "                \n",
    "                if(last != None):\n",
    "                    connection = last[len(last)//2:] + seq[0][:len(seq[0])//2]\n",
    "                    encoder, decoder, output = createInputTarget(connection)\n",
    "                    inputSentences.extend(encoder)\n",
    "                    targetSentences.extend(decoder)\n",
    "                    outputSentences.extend(output)\n",
    "                \n",
    "                last = deepcopy(seq[-1])\n",
    "                \n",
    "                \n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}.csv'.format(\n",
    "EPOCHS, \n",
    "SENTENCES_MAX_LENGTH, \n",
    "BATCH_SIZE, \n",
    "EMBEDDING_DIM,\n",
    "HIDDEN_DIM)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "#df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the purpose of the padding token?\n",
    "```python\n",
    "'<PAD>' : 0\n",
    "```\n",
    "\n",
    "In order to be able to feed the model we need to create inputs of the same length.\n",
    "This is way I defined a function to generate final data with paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "        if(i%10000 == 0):print('Generating feeding data... {}/{}'.format(i,numSamples))    \n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract embeddings matrix\n",
    "Loading pre-trained embeddings is a good practice to use them and in this case I calculated them with Google's Word2Vec model on the famous text8 dataset.\n",
    "- *More details on __train_embeddings.ipyn__ notebook* (To be executed if the .bin file do not exists)\n",
    "\n",
    "The embeddings are simply 128 (or whatever is the dimensionality during training) weigths from a single neuron in the input layer to the 128 neurons in the hidden layer trained to understand which words compared in the same context for a given text.\n",
    "\n",
    "So we simply extract these weights for every single word in our vocabulary and build a matrix with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Or it is possible to use random weights_\n",
    "Do not execute this cell to use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the model\n",
    "To build the transformer model I use and external library available on <a href=\"https://github.com/kpot/keras-transformer\">this GitHub repository</a>.\n",
    "The the model is trained and its weight are saved in a .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "            [encoderInputData, decoderInputData],\n",
    "            decoderTargetData,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate text\n",
    "To conclude, here the prediction script, which will use the decode function from the open source library to predict the next word again and again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Wolf',\n",
    "    'There was once a little Bear', \n",
    "    'An eagle was given permission to fly over the country.',\n",
    "    'A dog was talking to a bear asking for some food. The bear who was hungry too said no.',\n",
    "    'There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.'\n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in encoderwords): \n",
    "            encoderwords.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPath = os.path.join(dirname, 'output_data/out_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
