{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - Importing needed dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "from copy import deepcopy\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Declaring global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS = \"CONTINOUS_ONE\" # CONTINOUS or DOTS\n",
    "EPOCHS = 100\n",
    "SENTENCES_MAX_LENGTH = 100\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "ENCODERS = 1\n",
    "DECODERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "HEADS_ATTENTION = 8\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Try read data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n",
      "147 fables cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    cleanedFables = []\n",
    "    for f in fables:\n",
    "        cleanedFables.append(clean(f))\n",
    "    \n",
    "    print('{} fables cleaned.'.format(len(cleanedFables)))\n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "\n",
    "for fable in cleanedFables:\n",
    "    words = fable.split(' ')\n",
    "    s += len(words)\n",
    "\n",
    "mean = s/len(cleanedFables)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Extract vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<START>', '<END>']\n",
      "Vocabulary Size: 3063\n"
     ]
    }
   ],
   "source": [
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "\n",
    "for fable in fables:\n",
    "    words = fable.split(' ')\n",
    "\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "print(idx2word[:3])\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 25707\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "def createInputTarget(words) :\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    output = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        encode_tokens, decode_tokens = words[:i], words[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        encoder.append(encode_tokens)\n",
    "        decoder.append(decode_tokens)\n",
    "        output.append(output_tokens)\n",
    "        \n",
    "    return encoder, decoder, output\n",
    "\n",
    "def getWordTokens(sentence):\n",
    "    #clean tokens\n",
    "    words = sentence.split(' ')\n",
    "    words.append('.')\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    return words\n",
    "\n",
    "def checkMaxLength(words):\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    if len(words) > SENTENCES_MAX_LENGTH :\n",
    "        seq.append(words[:SENTENCES_MAX_LENGTH])\n",
    "        seq.append(words[SENTENCES_MAX_LENGTH:])\n",
    "        while len(seq[-1]) > SENTENCES_MAX_LENGTH:\n",
    "            tmp = deepcopy(seq[-1])\n",
    "            del seq[-1]\n",
    "            seq.append(tmp[:SENTENCES_MAX_LENGTH])\n",
    "            seq.append(tmp[SENTENCES_MAX_LENGTH:])\n",
    "    else : \n",
    "        seq.append(words)\n",
    "\n",
    "    return seq\n",
    "\n",
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "if PREPROCESS == 'CONTINOUS':\n",
    "    \n",
    "    for fable in cleanedFables:\n",
    "        words = fable.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+SENTENCES_MAX_LENGTH] for i in range(0, len(words), SENTENCES_MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "elif PREPROCESS == 'CONTINOUS_ONE':\n",
    "    \n",
    "    for fable in cleanedFables:\n",
    "        words = fable.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+SENTENCES_MAX_LENGTH] for i in range(0, len(words), SENTENCES_MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            l = len(s)\n",
    "            for i in range(1, l):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens )\n",
    "                output_tokens = decode_tokens\n",
    "                decode_tokens = ' '.join(['<START>'] + [decode_tokens] )\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "        \n",
    "            encode_tokens = ' '.join(['<START>'] + s )\n",
    "            output_tokens = '<END>'\n",
    "            decode_tokens = ' '.join(['<START>'] + ['<END>'])\n",
    "            inputSentences.append(encode_tokens)\n",
    "            targetSentences.append(decode_tokens)\n",
    "            outputSentences.append(output_tokens)\n",
    "                           \n",
    "                \n",
    "\n",
    "elif PREPROCESS == 'DOTS' : \n",
    "    \n",
    "    for fable in cleanedFables :\n",
    "        sentences = fable.split('.')\n",
    "        \n",
    "        last = None;\n",
    "        \n",
    "        for idx, s in enumerate(sentences):\n",
    "            \n",
    "            words = getWordTokens(s)\n",
    "            \n",
    "            if(len(words) > 2):\n",
    "\n",
    "                seq = checkMaxLength(words)\n",
    "                \n",
    "                for s1 in seq:\n",
    "                    if(len(s1) > 2):\n",
    "                        encoder, decoder, output = createInputTarget(s1)\n",
    "                        inputSentences.extend(encoder)\n",
    "                        targetSentences.extend(decoder)\n",
    "                        outputSentences.extend(output)\n",
    "                \n",
    "                if(last != None):\n",
    "                    connection = last[len(last)//2:] + seq[0][:len(seq[0])//2]\n",
    "                    encoder, decoder, output = createInputTarget(connection)\n",
    "                    inputSentences.extend(encoder)\n",
    "                    targetSentences.extend(decoder)\n",
    "                    outputSentences.extend(output)\n",
    "                \n",
    "                last = deepcopy(seq[-1])\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "EPOCHS, \n",
    "SENTENCES_MAX_LENGTH, \n",
    "BATCH_SIZE, \n",
    "EMBEDDING_DIM,\n",
    "HIDDEN_DIM,\n",
    "NUM_TRAIN_PLOTS)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "#df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Data generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "            \n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 128),  392064      Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 128)    263296      Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 128)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 128)    263296      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 128)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3063)   3063        Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 1,121,143\n",
      "Trainable params: 729,079\n",
      "Non-trainable params: 392,064\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "25707/25707 [==============================] - 41s 2ms/step - loss: 3.4247\n",
      "Epoch 2/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 3.1588\n",
      "Epoch 3/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 3.0559\n",
      "Epoch 4/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.9896\n",
      "Epoch 5/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.9405\n",
      "Epoch 6/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.8948\n",
      "Epoch 7/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.8670\n",
      "Epoch 8/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.8264\n",
      "Epoch 9/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.8025\n",
      "Epoch 10/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.7813\n",
      "Epoch 11/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.7578\n",
      "Epoch 12/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.7352\n",
      "Epoch 13/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.7134\n",
      "Epoch 14/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.6937\n",
      "Epoch 15/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.6769\n",
      "Epoch 16/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.6648\n",
      "Epoch 17/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.6550\n",
      "Epoch 18/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.6398\n",
      "Epoch 19/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.6286\n",
      "Epoch 20/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.6199\n",
      "Epoch 21/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.6081\n",
      "Epoch 22/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5958\n",
      "Epoch 23/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5865\n",
      "Epoch 24/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5810\n",
      "Epoch 25/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5686\n",
      "Epoch 26/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5579\n",
      "Epoch 27/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5501\n",
      "Epoch 28/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5428\n",
      "Epoch 29/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5336\n",
      "Epoch 30/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5276\n",
      "Epoch 31/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5179\n",
      "Epoch 32/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5080\n",
      "Epoch 33/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.5002\n",
      "Epoch 34/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4903\n",
      "Epoch 35/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4839\n",
      "Epoch 36/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.4762\n",
      "Epoch 37/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.4631\n",
      "Epoch 38/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4576\n",
      "Epoch 39/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.4463\n",
      "Epoch 40/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4392\n",
      "Epoch 41/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.4342\n",
      "Epoch 42/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4239\n",
      "Epoch 43/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4160\n",
      "Epoch 44/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.4078\n",
      "Epoch 45/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3984\n",
      "Epoch 46/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3902\n",
      "Epoch 47/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3831\n",
      "Epoch 48/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.3677\n",
      "Epoch 49/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3659\n",
      "Epoch 50/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3547\n",
      "Epoch 51/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3437\n",
      "Epoch 52/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.3362\n",
      "Epoch 53/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3280\n",
      "Epoch 54/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3231\n",
      "Epoch 55/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3108\n",
      "Epoch 56/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.3010\n",
      "Epoch 57/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2952\n",
      "Epoch 58/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2827\n",
      "Epoch 59/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2759\n",
      "Epoch 60/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2604\n",
      "Epoch 61/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2538\n",
      "Epoch 62/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.2485\n",
      "Epoch 63/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2383\n",
      "Epoch 64/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2333\n",
      "Epoch 65/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2203\n",
      "Epoch 66/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.2135\n",
      "Epoch 67/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.1993\n",
      "Epoch 68/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.1940\n",
      "Epoch 69/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1920\n",
      "Epoch 70/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1738\n",
      "Epoch 71/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1628\n",
      "Epoch 72/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1587\n",
      "Epoch 73/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.1466\n",
      "Epoch 74/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1320\n",
      "Epoch 75/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1330\n",
      "Epoch 76/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1164\n",
      "Epoch 77/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.1122\n",
      "Epoch 78/100\n",
      "25707/25707 [==============================] - 39s 2ms/step - loss: 2.1045\n",
      "Epoch 79/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.0888\n",
      "Epoch 80/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0790\n",
      "Epoch 81/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0704\n",
      "Epoch 82/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0627\n",
      "Epoch 83/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0499\n",
      "Epoch 84/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0454\n",
      "Epoch 85/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0395\n",
      "Epoch 86/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0242\n",
      "Epoch 87/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0192\n",
      "Epoch 88/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 2.0074\n",
      "Epoch 89/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 2.0053\n",
      "Epoch 90/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9932\n",
      "Epoch 91/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9809\n",
      "Epoch 92/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9725\n",
      "Epoch 93/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 1.9691\n",
      "Epoch 94/100\n",
      "25707/25707 [==============================] - 39s 1ms/step - loss: 1.9604\n",
      "Epoch 95/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9603\n",
      "Epoch 96/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9487\n",
      "Epoch 97/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9344\n",
      "Epoch 98/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9234\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9179\n",
      "Epoch 100/100\n",
      "25707/25707 [==============================] - 38s 1ms/step - loss: 1.9132\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "            [encoderInputData, decoderInputData],\n",
    "            decoderTargetData,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - Generate sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: The Cock\n",
      "Generated:  <START> and <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Generating from: A Dog and a Wolf\n",
      "Generated:  <START> ago <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Generating from: There was once a little Bear\n",
      "Generated:  <START> the <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Generating from: An eagle was given permission to fly over the country.\n",
      "Generated:  <START> at <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Generating from: A dog was talking to a bear asking for some food. The bear who was hungry too said no.\n",
      "Generated:  <START> i <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Generating from: There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.\n",
      "Generated:  <START> the <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Wolf',\n",
    "    'There was once a little Bear', \n",
    "    'An eagle was given permission to fly over the country.',\n",
    "    'A dog was talking to a bear asking for some food. The bear who was hungry too said no.',\n",
    "    'There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.'\n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in encoderwords): \n",
    "            encoderwords.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPath = os.path.join(dirname, 'output_data/out_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
