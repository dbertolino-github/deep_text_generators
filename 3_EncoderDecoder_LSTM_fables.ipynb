{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies \n",
    "Tensorflow background session is launched to define GPU settings.\n",
    "In this first step we also define all global variables that will help managing redundancy:\n",
    "\n",
    "- __*EPOCHS*__: number of epochs in which the training is divided.\n",
    "- __*MAX_LENGTH*__: Maximum length of the variable dimension phrases..\n",
    "- __*BATCH_SIZE*__: number of samples after which update the weights.\n",
    "- __*EMBEDDING_DIM*__: number of neurons in the Embeddings layer.\n",
    "- __*RNN_DIM*__: number of LSTM units in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "EPOCHS = 2\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Aesop fables data\n",
    "The chosen dataset is a JSON file containing 147 Aesop Fables divided in sentences.\n",
    "For the availabilty, I need to to thanks this funny and interesting project on Aesop Fables which explore the connections between them using machine learning: <a href=\"https://github.com/itayniv/aesop-fables-stories\">GitHub repository</a>\n",
    "\n",
    "Here an example of how it is structured:\n",
    "```json\n",
    "{\n",
    "  \"stories\":[\n",
    "    {\n",
    "      \"number\": \"01\",\n",
    "      \"title\": \"THE WOLF AND THE KID\",\n",
    "      \"story\": [\n",
    "        \"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself.\",\n",
    "        \"So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass.\",\n",
    "        \"A little later when he lifted his head, the flock was gone.\",\n",
    "        \"He was all alone.\",\n",
    "        \"The sun was sinking.\",\n",
    "        \"Long shadows came creeping over the ground.\",\n",
    "        \"A chilly little wind came creeping with them making scary noises in the grass.\",\n",
    "        \"The Kid shivered as he thought of the terrible Wolf.\",\n",
    "        \"Then he started wildly over the field, bleating for his mother.\",\n",
    "        \"But not half-way, near a clump of trees, there was the Wolf!\",\n",
    "        \"The Kid knew there was little hope for him.\",\n",
    "        \"Please, Mr. Wolf, he said trembling, I know you are going to eat me.\",\n",
    "        \"But first please pipe me a tune, for I want to dance and be merry as long as I can.\",\n",
    "        \"The Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily.\",\n",
    "        \"Meanwhile, the flock was moving slowly homeward.\",\n",
    "        \"In the still evening air the Wolf's piping carried far.\",\n",
    "        \"The Shepherd Dogs pricked up their ears.\",\n",
    "        \"They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture.\",\n",
    "        \"The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade.\"\n",
    "      ],\n",
    "      \"moral\": \"Do not let anything turn you from your purpose.\",\n",
    "      \"characters\": []\n",
    "    }, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n",
      "147 fables cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    cleanedFables = []\n",
    "    for f in fables:\n",
    "        cleanedFables.append(clean(f))\n",
    "    \n",
    "    print('{} fables cleaned.'.format(len(cleanedFables)))\n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to investigate on fables max length and average length to better decided preprocess hyperparamateres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fable length: 205.40816326530611\n",
      "Maximum fable length: 549\n"
     ]
    }
   ],
   "source": [
    "sumLen = 0\n",
    "maxLen = 0\n",
    "for f in cleanedFables:\n",
    "    words = f.split(' ')\n",
    "    thisLen = len(words)\n",
    "    sumLen += thisLen\n",
    "    if thisLen > maxLen:\n",
    "        maxLen = thisLen\n",
    "\n",
    "avgLen = sumLen/len(cleanedFables)\n",
    "\n",
    "print('Average fable length: {}'.format(avgLen))\n",
    "print('Maximum fable length: {}'.format(maxLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Vocabulary\n",
    "The vocabulary is saved as: \n",
    "- a __numpy array__ to map each encoding to the right word\n",
    "- a __dictionary__ to map each word to its encoding number \n",
    "\n",
    "We also create a __textAsInt__ variable that contains all fables text encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3062\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "wordSequence = []\n",
    "for fable in cleanedFables:\n",
    "    words = fable.split(' ')\n",
    "    wordSequence.extend(words)\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "# textAsInt = np.array([word2idx[w] for w in wordSequence])\n",
    "vocab_size = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess text\n",
    "\n",
    "Thanks to Encoder-Decoder architecture we can now train the model to generate variable dimension sequences, meaning that it will be the model itself to decide how many words have to be generated for a determined input sequence.\n",
    "However in order to achieve this result the text has to preprocessed in a way that let the model undesrstand where a sequence starts and where it ends.\n",
    "In fact in the previous code cell we had these three tokens to the vocabulary:\n",
    "\n",
    "```python\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "```\n",
    "\n",
    "We're going to divide the text into sequences of words, respecting a maximum length decided a priori.\n",
    "Each sequence will generate as many samples as its number of words.\n",
    "\n",
    "For example, say SEQUENCES_LENGTH is 4 and our text is \"Hello my name is Dario and I love to code\". \n",
    "- Sequences: \"Hello my name is \", \"Dario and I love\", \"to code\"\n",
    "\n",
    "Then with the first sequence:\n",
    "- __EncoderInput__: \"START Hello END\" <br/>\n",
    "  __DecoderInput__: \"START my name is END\" <br/>\n",
    "  __Target__: \"my name is END\" <br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my END\" <br/>\n",
    "  __DecoderInput__: \"START name is END\"<br/>\n",
    "  __Target__: \"name is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__:  \"START Hello my name END\"<br/>\n",
    "  __DecoderInput__: \"START is END\"<br/>\n",
    "  __Target__: \"is END\"<br/>\n",
    "  \n",
    "  \n",
    "- __EncoderInput__: \"START Hello my name is END\" <br/>\n",
    "  __DecoderInput__: \"START END\"<br/>\n",
    "  __Target__: \"END\"<br/>\n",
    "\n",
    "\n",
    "\n",
    "In this way, sequences of every length can be preprocessed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 26809\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "for fable in cleanedFables:\n",
    "        words = fable.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+MAX_LENGTH] for i in range(0, len(words), MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_fables_{}_{}.csv'.format(\n",
    "MAX_LENGTH,  \n",
    "BATCH_SIZE)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the purpose of the padding token?\n",
    "```python\n",
    "'<PAD>' : 0\n",
    "```\n",
    "\n",
    "In order to be able to feed the model we need to create inputs of the same length.\n",
    "This is way I defined a function to generate final data with paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_ed_fables_{}_{}.csv'.format(\n",
    "    MAX_LENGTH,  \n",
    "    BATCH_SIZE)))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "        if(i%10000 == 0):print('Generating feeding data... {}/{}'.format(i,numSamples))\n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract embeddings matrix\n",
    "Loading pre-trained embeddings is a good practice to use them and in this case I calculated them with Google's Word2Vec model on the famous text8 dataset.\n",
    "- *More details on __train_embeddings.ipyn__ notebook* (To be executed if the .bin file do not exists)\n",
    "\n",
    "The embeddings are simply 128 (or whatever is the dimensionality during training) weigths from a single neuron in the input layer to the 128 neurons in the hidden layer trained to understand which words compared in the same context for a given text.\n",
    "\n",
    "So we simply extract these weights for every single word in our vocabulary and build a matrix with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Or it is possible to use random weights_\n",
    "Do not execute this cell to use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the model\n",
    "\n",
    "The encoder simply takes the input data, and train on it then it passes the last state of its recurrent layer as an initial state to the first recurrent layer of the decoder part.\n",
    "So the encoder model will be a simple Neural Network composed by:\n",
    "- Input layer\n",
    "- Embeddings layer \n",
    "- Recurrent Layer (Long Short Memory Network)\n",
    "\n",
    "The decoder takes the last state of encoder’s last recurrent layer and uses it as an initial state to its first recurrent layer , the input of the decoder is the sequences that we want to get.\n",
    "So the decoder model will be a simple Neural Network composed by:\n",
    "- Input layer\n",
    "- Embeddings layer \n",
    "- Recurrent Layer (Long Short Memory Network)\n",
    "- Dense output (to predict the next word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(vocab_length, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Define an input sequence and process it.\n",
    "    # Input layer of the encoder :\n",
    "    encoderInput = Input(shape=(None,))\n",
    "    \n",
    "    # Hidden layers of the encoder :\n",
    "    encoder_embedding = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])(encoderInput)\n",
    "\n",
    "    # Output layer of the encoder :\n",
    "    encoder_LSTM = CuDNNLSTM(hidden_dim , return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoderStates = [state_h, state_c]\n",
    "    \n",
    "    \n",
    "    return encoderInput, encoderStates\n",
    "\n",
    "\n",
    "def build_encoder_gen(encoder_input, encoder_states):\n",
    "    '''\n",
    "    '''\n",
    "    encoderModelGen = Model(encoder_input, encoder_states)\n",
    "\n",
    "    return encoderModelGen\n",
    "\n",
    "\n",
    "def build_decoder(vocab_length, encoderStates, embedding_weigths=embeddingMatrix, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    # Set up the decoder, using `encoderStates` as initial state.\n",
    "    # Input layer of the decoder :\n",
    "    decoderInput = Input(shape=(None,))\n",
    "\n",
    "    # Hidden layers of the decoder :\n",
    "    decoderEmbeddingLayer = Embedding(input_dim = vocab_length, output_dim = embedding_dim, weights=[embedding_weigths])\n",
    "    decoder_embedding = decoderEmbeddingLayer(decoderInput)\n",
    "\n",
    "    decoderLSTMLayer = CuDNNLSTM(hidden_dim , return_sequences=True, return_state=True)\n",
    "    decoder_LSTM_output, _ , _ = decoderLSTMLayer(decoder_embedding, initial_state=encoderStates)\n",
    "\n",
    "    # Output layer of the decoder :\n",
    "    decoderDenseLayer = Dense(vocab_length, activation='softmax')\n",
    "    decoderOutput = decoderDenseLayer(decoder_LSTM_output)\n",
    "\n",
    "    return decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer\n",
    "\n",
    "\n",
    "def build_decoder_gen(decoder_input, decoder_embedding_layer, decoder_LSTM_layer, decoder_dense, hidden_dim=HIDDEN_DIM):\n",
    "    '''\n",
    "    '''\n",
    "    decoder_state_input_h = Input(shape=(hidden_dim,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_embedding_gen = decoder_embedding_layer(decoder_input)\n",
    "    decoder_LSTM_output_gen, state_h_gen , state_c_gen = decoder_LSTM_layer(decoder_embedding_gen, initial_state = decoder_states_inputs)\n",
    "    decoder_states_gen = [state_h_gen, state_c_gen]\n",
    "    decoderOutputGen = decoder_dense(decoder_LSTM_output_gen)\n",
    "\n",
    "    # sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\n",
    "    decoderModelGen = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoderOutputGen] + decoder_states_gen\n",
    "    )\n",
    "\n",
    "    return decoderModelGen\n",
    "  \n",
    "def build_encoder_decoder_model(encoder_input, decoder_input, decoder_output):\n",
    "    '''\n",
    "    '''\n",
    "    model = Model([encoder_input, decoder_input], decoder_output)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the model\n",
    "We train the model and save encoder and decoder weigths in two separeted .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    391936      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    391936      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          [(None, 1024), (None 4726784     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, None, 1024), 4726784     embedding_1[0][0]                \n",
      "                                                                 cu_dnnlstm[0][1]                 \n",
      "                                                                 cu_dnnlstm[0][2]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 3062)   3138550     cu_dnnlstm_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 13,375,990\n",
      "Trainable params: 13,375,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Generating feeding data... 0/26809\n",
      "Generating feeding data... 10000/26809\n",
      "Generating feeding data... 20000/26809\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/2\n",
      "26809/26809 [==============================] - 113s 4ms/sample - loss: 0.9110\n",
      "Epoch 2/2\n",
      "26809/26809 [==============================] - 111s 4ms/sample - loss: 0.1717\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "encoderInput, encoderStates = build_encoder(vocab_length=vocab_size)\n",
    "\n",
    "decoderInput, decoderOutput, decoderEmbeddingLayer,  decoderLSTMLayer, decoderDenseLayer = build_decoder(\n",
    "    vocab_length=vocab_size, \n",
    "    encoderStates=encoderStates\n",
    ")\n",
    "\n",
    "model = build_encoder_decoder_model(\n",
    "    encoder_input=encoderInput, \n",
    "    decoder_input=decoderInput, \n",
    "    decoder_output=decoderOutput\n",
    ")\n",
    "\n",
    "encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "    word_2_idx=word2idx,\n",
    "    num_samples=numSamples,\n",
    "    max_length=MAX_LENGTH, \n",
    "    vocab_length=vocab_size\n",
    ")\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.fit([encoderInputData, decoderInputData], decoderTargetData, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "encoderModelGen = build_encoder_gen(\n",
    "    encoder_input = encoderInput, \n",
    "    encoder_states = encoderStates\n",
    ")\n",
    "\n",
    "decoderModelGen = build_decoder_gen(\n",
    "    decoder_input = decoderInput, \n",
    "    decoder_embedding_layer = decoderEmbeddingLayer, \n",
    "    decoder_LSTM_layer = decoderLSTMLayer, \n",
    "    decoder_dense = decoderDenseLayer\n",
    ")\n",
    "\n",
    "encoderModelGen.save(encoderGenPath)\n",
    "decoderModelGen.save(decoderGenPath)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate text\n",
    "To conclude, here the prediction script, which will initialize decoder with seed and then let it predict the next word again and again\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "GENEREATING FROM: the cock\n",
      "GENERATED:  mouse , who had to take them to the found , there he was able to do . lifting his wings he tried to rise from the ground . but the weight of his magnificent train held him down . instead of flying up to greet the first rays of the morning sun or to bathe in the rosy light among the floating clouds at sunset , he would have to walk the ground more encumbered and oppressed than any common barnyard fowl . <END>\n",
      "GENEREATING FROM: a dog and a wolf\n",
      "GENERATED:  tree , and the animals respectfully made way for him , an ass brayed a scornful remark as he passed . the lion felt a flash of anger . but when he turned his head and saw who had spoken , he walked quietly on . he would not honor the fool with even so much as a stroke of his claws . <END>\n",
      "GENEREATING FROM: there was once a little bear\n",
      "GENERATED:  , and there he was with a house on his back and little short legs that could hardly drag him along . one day he met a pair of ducks and told them all his trouble . we can help you to see the world , said the ducks . take hold of this stick with your teeth and we will carry you far up in the air where you can see the whole countryside . but keep quiet or <END>\n",
      "GENEREATING FROM: an eagle was given permission to fly over the country . \n",
      "GENERATED:  when the fox saw the mouse , he swam to the bank and croaked : won you pay me a visit ? i can promise you a good time if you do . the mouse did not need much coaxing , for he was very anxious to see the world and everything in it . but though he could swim a little , he did not dare risk going into the pond without some help . the frog had a <END>\n",
      "GENEREATING FROM: a dog was talking to a bear asking for some food .  the bear who was hungry too said no . \n",
      "GENERATED:  they had so jupiter to do that any animal that he chose for a meal , should be so brazen as to wear such dangerous things as horns to scratch him while he ate . so he commanded that all animals with horns should leave his domains within twentyfour hours . the command struck terror among the beasts . all those who were so unfortunate as to have horns , began to pack up and move out . even the hare , <END>\n",
      "GENEREATING FROM: there was once a little mouse who walking in the forest .  he found his way into a bear cave .  it was alone and afraid .  the cave was really dark and the bear was sleeping . \n",
      "GENERATED:  and when he came out of the warren in the early morning sunshine , and there saw the shadow cast by his long and pointed ears , a terrible fright seized him . goodby , neighbor cricket , he called . im off . he will certainly make out that my ears are horns , no matter what i say . <END>\n"
     ]
    }
   ],
   "source": [
    "def generate_text(sentences, encoder_model, decoder_model, vocab_length, word_2_idx, idx_2_word, max_length):\n",
    "    '''\n",
    "    '''\n",
    "    for phrase in sentences:\n",
    "\n",
    "        # Cleaning sentence\n",
    "        phrase = clean(phrase)\n",
    "        print('GENEREATING FROM: {}'.format(phrase))\n",
    "        tokens = phrase.split(' ')\n",
    "        inputSequence = np.zeros((1, max_length), dtype='int')\n",
    "        for i, t in enumerate(tokens):\n",
    "            inputSequence[0, i] = word_2_idx[t]\n",
    "\n",
    "        # Encode the input as state vectors.\n",
    "        statesValue = encoder_model.predict(inputSequence)\n",
    "        # Generate empty target sequence of length 1.\n",
    "        targetSeq = np.zeros((1, 1))\n",
    "        targetSeq[0, 0] = word_2_idx['<START>']\n",
    "        # Sampling loop for a batch of sequences\n",
    "        # (to simplify, here we assume a batch of size 1).\n",
    "        stopCondition = False\n",
    "        decodedSentence = ''\n",
    "        decodedList = []\n",
    "        while not stopCondition:\n",
    "            outputTokens, h, c = decoder_model.predict(\n",
    "                [targetSeq] + statesValue)\n",
    "\n",
    "            # Sample a token\n",
    "            sampledTokenIndex = np.argmax(outputTokens[0, -1, :])\n",
    "            sampledWord = idx_2_word[sampledTokenIndex]\n",
    "            decodedList.append(sampledWord)\n",
    "            decodedSentence += ' ' + sampledWord\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if (sampledWord == '<END>' or len(decodedList)== max_length):\n",
    "                stopCondition = True\n",
    "\n",
    "            # Update the target sequence (of length 1).\n",
    "            targetSeq = np.zeros((1, 1))\n",
    "            targetSeq[0, 0] = sampledTokenIndex\n",
    "\n",
    "            # Update states\n",
    "            statesValue = [h, c]\n",
    "\n",
    "        print('GENERATED: {}'.format(decodedSentence))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "        \n",
    "dirname = os.path.abspath('')\n",
    "\n",
    "encoderGenPath = os.path.join(dirname, 'models/encoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "decoderGenPath = os.path.join(dirname, 'models/decoder_fables_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM)\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Wolf',\n",
    "    'There was once a little Bear',\n",
    "    'An eagle was given permission to fly over the country.',\n",
    "    'A dog was talking to a bear asking for some food. The bear who was hungry too said no.',\n",
    "    'There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.'\n",
    "]\n",
    "\n",
    "encoderModel = load_model(encoderGenPath)\n",
    "encoderModel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "decoderModel = load_model(decoderGenPath)\n",
    "decoderModel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "generate_text(\n",
    "    sentences = sentences,\n",
    "    encoder_model = encoderModel,\n",
    "    decoder_model = decoderModel, \n",
    "    vocab_length = vocab_size, \n",
    "    word_2_idx = word2idx, \n",
    "    idx_2_word = idx2word, \n",
    "    max_length = MAX_LENGTH\n",
    ")\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
