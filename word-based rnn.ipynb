{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import dependencies and decleare global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEQUENCES_LENGTH = 30\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_DIM = 1024 \n",
    "NUM_PLOTS = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import fables data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/wiki_movie_plots.csv')\n",
    "    dataframe = pd.read_csv(filepath, sep=',')\n",
    "    plotList = dataframe['Plot']\n",
    "    plotList = plotList[:NUM_PLOTS]\n",
    "    plotText = ''\n",
    "    print('{} plots imported.'.format(len(plotList)))\n",
    "    for idx , p in enumerate(plotList):\n",
    "        cleaned = clean(p)\n",
    "        plotText += ' ' + cleaned + '\\n'\n",
    "    \n",
    "    print('{} plots cleaned.'.format(len(plotList)))\n",
    "    \n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "word2idx = {'<PAD>' : 0, '<START>' : 1 , '<END>': 2}\n",
    "wordSequence = []\n",
    "for plot in plotList:\n",
    "    words = plot.split(' ')\n",
    "    wordSequence.extend(words)\n",
    "    for word in words:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = list(word2idx.keys())\n",
    "textAsInt = np.array([word2idx[w] for w in wordSequence])\n",
    "vocab_size = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    '''\n",
    "    '''\n",
    "    inputText = chunk[:-1]\n",
    "    targetText = chunk[1:]\n",
    "    return inputText, targetText\n",
    "\n",
    "wordDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "sequences = wordDataset.batch(SEQUENCES_LENGTH+1, drop_remainder=True) #The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "examplesPerEpoch = len(wordSequence) // SEQUENCES_LENGTH\n",
    "stepsPerEpoch = examplesPerEpoch // BATCH_SIZE\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  '''\n",
    "  '''\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits)\n",
    "\n",
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "trainModel = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, weigths=[embeddingMatrix], batch_input_shape=[BATCH_SIZE, None]),\n",
    "    rnn(RNN_DIM,return_sequences=True, recurrent_initializer='glorot_uniform',stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)]\n",
    ")\n",
    "\n",
    "trainModel.summary()\n",
    "\n",
    "trainModel.compile(\n",
    "      optimizer = tf.train.AdamOptimizer(),\n",
    "      loss = loss)\n",
    "\n",
    "trainModel.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=stepsPerEpoch)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_word_fables.h5')\n",
    "trainModel.save_weights(weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define generation model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "genModel = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, \n",
    "    batch_input_shape=[1, None]),\n",
    "    rnn(RNN_DIM,return_sequences=True, recurrent_initializer='glorot_uniform',stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)]\n",
    ")\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_word_fables.h5')\n",
    "genModel.load_weights(weightsPath)\n",
    "genModel.build(tf.TensorShape([1, None]))\n",
    "genModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, word_2_idx, idx_2_word):\n",
    "    '''\n",
    "    '''\n",
    "    # Evaluation step (generating text using the learned weights)\n",
    "    # Number of characters to generate\n",
    "    numGenerate = 30\n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    s = clean(start_string)\n",
    "    inputEval = [word_2_idx[w] for w in s.split(' ')]\n",
    "    inputEval = tf.expand_dims(inputEval, 0)\n",
    "    # Empty string to store our results\n",
    "    textGenerated = []\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(numGenerate):\n",
    "        predictions = model(inputEval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a multinomial distribution to predict the word returned by the trainModel\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].eval(session = session)\n",
    "        # We pass the predicted word as the next input to the trainModel\n",
    "        # along with the previous hidden state\n",
    "        inputEval = tf.expand_dims([predicted_id], 0)\n",
    "        textGenerated.append(idx_2_word[predicted_id])\n",
    "\n",
    "    return (start_string + ' '.join(textGenerated))\n",
    "\n",
    "generated = generate_text(\n",
    "        model=genModel, \n",
    "        start_string=\"There was once a little\", \n",
    "        word_2_idx=word2idx, \n",
    "        idx_2_word=idx2word\n",
    "    )\n",
    "\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
