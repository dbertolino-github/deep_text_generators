{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import dependencies and decleare global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEQUENCES_LENGTH = 30\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_DIM = 1024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import fables data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    fablesText = ''\n",
    "    for idx, f in enumerate(fables):\n",
    "        fablesText = fablesText + ' ' + clean(f) + '\\n'\n",
    "    \n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3061\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = []\n",
    "wordSequence = fablesText.split(' ')\n",
    "\n",
    "b=True\n",
    "while b:\n",
    "    if('' in wordSequence): \n",
    "        wordSequence.remove('')\n",
    "    else: b = False\n",
    "\n",
    "for word in wordSequence:\n",
    "    if word not in idx2word:\n",
    "        idx2word.append(word)\n",
    "\n",
    "word2idx = {}\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "vocab_size = len(idx2word)\n",
    "print('Vocabulary size: {}'.format(vocab_size))\n",
    "\n",
    "textAsInt = np.array([word2idx[w] for w in wordSequence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    '''\n",
    "    '''\n",
    "    inputText = chunk[:-1]\n",
    "    targetText = chunk[1:]\n",
    "    return inputText, targetText\n",
    "\n",
    "wordDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "sequences = wordDataset.batch(SEQUENCES_LENGTH+1, drop_remainder=True) #The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "examplesPerEpoch = len(fablesText) // SEQUENCES_LENGTH\n",
    "stepsPerEpoch = examplesPerEpoch // BATCH_SIZE\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  '''\n",
    "  '''\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits)\n",
    "\n",
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "trainModel = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, \n",
    "    batch_input_shape=[BATCH_SIZE, None]),\n",
    "    rnn(RNN_DIM,return_sequences=True, recurrent_initializer='glorot_uniform',stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)]\n",
    ")\n",
    "\n",
    "trainModel.summary()\n",
    "\n",
    "trainModel.compile(\n",
    "      optimizer = tf.train.AdamOptimizer(),\n",
    "      loss = loss)\n",
    "\n",
    "trainModel.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=stepsPerEpoch)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_word_fables.h5')\n",
    "trainModel.save_weights(weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define generation model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "genModel = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, \n",
    "    batch_input_shape=[1, None]),\n",
    "    rnn(RNN_DIM,return_sequences=True, recurrent_initializer='glorot_uniform',stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)]\n",
    ")\n",
    "\n",
    "genModel.load_weights(weightsPath)\n",
    "genModel.build(tf.TensorShape([1, None]))\n",
    "genModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, char_2_idx, idx_2_char):\n",
    "    '''\n",
    "    '''\n",
    "    # Evaluation step (generating text using the learned weights)\n",
    "    # Number of characters to generate\n",
    "    numGenerate = SEQUENCES_LENGTH\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    start_string = clean(start_string) \n",
    "    inputEval = [char_2_idx[s] for s in start_string]\n",
    "    inputEval = tf.expand_dims(inputEval, 0)\n",
    "    # Empty string to store our results\n",
    "    textGenerated = []\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(numGenerate):\n",
    "        predictions = model(inputEval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a multinomial distribution to predict the word returned by the trainModel\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        # We pass the predicted word as the next input to the trainModel\n",
    "        # along with the previous hidden state\n",
    "        inputEval = tf.expand_dims([predicted_id], 0)\n",
    "        textGenerated.append(idx_2_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(textGenerated))\n",
    "\n",
    "generated = generate_text(\n",
    "        model=genModel, \n",
    "        start_string=\"There was once a little\", \n",
    "        word_2_idx=word2idx, \n",
    "        idx_2_word=idx2word\n",
    "    )\n",
    "\n",
    "generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
