{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \\n \")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON TEXT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854 WORDS \n",
      "Printing first 100:\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'english', 'revolution', 'and', 'sans', 'culottes', 'french', 'whilst', 'is', 'still', 'in', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'violent', 'means', 'destroy', 'organization', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'word', 'derived', 'from', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'political', 'philosophy', 'belief', 'rulers', 'are', 'unnecessary', 'should', 'be', 'abolished', 'although', 'there', 'differing', 'interpretations', 'what', 'this', 'refers', 'related', 'social', 'movements', 'advocate', 'elimination', 'authoritarian', 'institutions', 'particularly', 'state', 'anarchy', 'most', 'use', 'does', 'not', 'imply', 'chaos', 'nihilism', 'or', 'anomie', 'but', 'rather', 'harmonious', 'anti', 'place']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./embeddings/text8_word2vec_skipgram_128.bin'):\n",
    "    print('Text8 Embeddings have already been trained')\n",
    "else :\n",
    "    corpus = api.load('text8')  # download the corpus and return it opened as an iterable\n",
    "    text8model = Word2Vec(corpus, min_count=1, sg=1, size=128)  # train a model from the corpus\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    text8model.save('embeddings/text8_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('having', 0.7466163635253906),\n",
       " ('have', 0.7401428818702698),\n",
       " ('has', 0.7261757850646973),\n",
       " ('hadn', 0.6986460089683533),\n",
       " ('eagerly', 0.6985013484954834),\n",
       " ('already', 0.6968131065368652),\n",
       " ('was', 0.6882036924362183),\n",
       " ('equalled', 0.6823857426643372),\n",
       " ('hasn', 0.6732666492462158),\n",
       " ('accidently', 0.6704608201980591)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text8model = gensim.models.Word2Vec.load('embeddings/text8_word2vec_skipgram_128.bin')\n",
    "text8model.most_similar(\"had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON WIKIPEDIA PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DATA READ\n",
    "\n",
    "if os.path.exists('./embeddings/plots_word2vec_skipgram_128.bin'):\n",
    "    print('Plots Embeddings have already been trained')\n",
    "else :\n",
    "    sentences = []\n",
    "    \n",
    "    for plot in trainPlotsList:\n",
    "        words = plot.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    plotsModel = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    plotsModel.save('embeddings/plots_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('has', 0.8246745467185974),\n",
       " ('have', 0.7286619544029236),\n",
       " ('was', 0.7282015085220337),\n",
       " ('hasn’t', 0.7263228297233582),\n",
       " ('previously', 0.7129716277122498),\n",
       " ('theyve', 0.6647764444351196),\n",
       " ('wouldve', 0.6559767723083496),\n",
       " ('shouldve', 0.6518896222114563),\n",
       " ('theyd', 0.6445757150650024),\n",
       " ('having', 0.6321873068809509)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotsModel = gensim.models.Word2Vec.load('embeddings/plots_word2vec_skipgram_128.bin')\n",
    "plotsModel.most_similar(\"had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON FABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n",
      "3061 WORDS \n",
      "Printing first 100:\n",
      "['there', 'was', 'once', 'a', 'little', 'kid', 'whose', 'growing', 'horns', 'made', 'him', 'think', 'he', 'grownup', 'billy', 'goat', 'and', 'able', 'to', 'take', 'care', 'of', 'himself', '.', '', 'so', 'one', 'evening', 'when', 'the', 'flock', 'started', 'home', 'from', 'pasture', 'his', 'mother', 'called', 'paid', 'no', 'heed', 'kept', 'right', 'on', 'nibbling', 'tender', 'grass', 'later', 'lifted', 'head', 'gone', 'all', 'alone', 'sun', 'sinking', 'long', 'shadows', 'came', 'creeping', 'over', 'ground', 'chilly', 'wind', 'with', 'them', 'making', 'scary', 'noises', 'in', 'shivered', 'as', 'thought', 'terrible', 'wolf', 'then', 'wildly', 'field', 'bleating', 'for', 'but', 'not', 'halfway', 'near', 'clump', 'trees', '!', 'knew', 'hope', 'please', 'mr', 'said', 'trembling', 'i', 'know', 'you', 'are', 'going', 'eat', 'me', 'first']\n"
     ]
    }
   ],
   "source": [
    "fables = []\n",
    "dirname = os.path.abspath('')\n",
    "filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "with open(filepath) as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for p in data['stories']:\n",
    "        fables.append(' '.join(p['story']))\n",
    "\n",
    "print('{} fables imported.'.format(len(fables)))\n",
    "\n",
    "for idx, f in enumerate(fables):\n",
    "    fables[idx] = clean(f)\n",
    "    \n",
    "if os.path.exists('./embeddings/fables_word2vec_skipgram_128.bin'):\n",
    "    print('Fables Embeddings have already been trained')\n",
    "else :\n",
    "    sentences = []\n",
    "    \n",
    "    for f in fables:\n",
    "        words = f.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    fablesModel = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    vocab = list(fablesModel.wv.vocab)\n",
    "    print('{} WORDS '.format(len(vocab)))\n",
    "    print('Printing first 100:')\n",
    "    print(vocab[:100])\n",
    "\n",
    "    fablesModel.save('embeddings/fables_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flock', 0.999713659286499),\n",
       " ('while', 0.9997000098228455),\n",
       " ('caught', 0.9996981024742126),\n",
       " ('near', 0.9996980428695679),\n",
       " ('home', 0.9996894598007202),\n",
       " ('pasture', 0.9996848702430725),\n",
       " ('stood', 0.9996843934059143),\n",
       " ('called', 0.999681293964386),\n",
       " ('over', 0.999680757522583),\n",
       " ('dogs', 0.9996804594993591)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fablesModel = gensim.models.Word2Vec.load('embeddings/fables_word2vec_skipgram_128.bin')\n",
    "fablesModel.most_similar(\"forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON BOOKING REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76905 WORDS \n",
      "Printing first 100:\n",
      "['parking', 'not', 'available', 'all', 'time', 'especially', 'in', 'the', 'evening', 'which', 'is', 'annoying', 'and', 'frustrating', 'smell', 'room', 'air', 'conditioning', 'system', 'situation', 'value', 'for', 'money', 'welcome', 'was', 'awesome', 'really', 'beautiful', 'chocolate', 'on', 'our', 'pillow', 'when', 'we', 'arrived', 'back', 'from', 'out', 'lovely', 'touches', 'magical', 'place', 'liked', 'everything', 'great', 'location', 'friendly', 'helpful', 'staff', 'poor', 'noise', 'insulation', 'very', 'loud', 'other', 'hotel', 'rooms', 'street', 'were', 'moved', 'to', 'a', 'higher', 'floor', 'after', 'complaining', 'where', 'it', 'slightly', 'better', 'breakfast', 'rather', 'lacking', 'pancakes', 'good', 'fruit', 'low', 'quality', 'close', 'bond', 'cleanliness', 'nice', 'shower', 'toilets', 'camp', 'nou', 'metro', 'area', 'doesn', 't', 'tend', 'cater', 'british', 'no', 'kettle', 'perhaps', 'bit', 'of', 'way', 'city']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    textReviewsData = pd.read_csv('input_data/text_reviews.csv', sep=',')\n",
    "except IOError:\n",
    "    sys.exit('Data not found')\n",
    "\n",
    "if os.path.exists('./embeddings/word2vec_skipgram_128.bin'):\n",
    "    print('Embeddings have already been trained')\n",
    "else :\n",
    "    text_reviews = textReviewsData['reviews_text'].values\n",
    "    sentences = []\n",
    "\n",
    "    for review in text_reviews:\n",
    "        cleaned = clean(review)\n",
    "        words = cleaned.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    model = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    model.save('embeddings/reviews_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON INTERNATIONAL BBC NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10510 news imported.\n",
      "30961 WORDS \n",
      "['aegon', 'life', 'iterm', 'insurance', 'plan', 'helps', 'you', 'save', 'tax', 'on', 'premiums', 'paid', 'for', 'over', 'a', 'century', '.', '', 'with', ',', 'get', 'cover', 'till', 'the', 'age', 'of', 'years', 'and', 'can', 'also', 'up', 'to', '₹', 'taxes', 'it', 'comes', 'critical', 'illness', 'rider', 'has', 'simple', 'claim', 'process', 'is', 'easy', 'renew', 'multiple', 'payout', 'options', 'an', 'yearold', 'woman', 'named', 'eileen', 'macken', 'who', 'grew', 'in', 'dublin', 'orphanage', 'said', 'she', 'not', 'orphan', 'anymore', 'after', 'discovering', 'her', 'mother', 'alive', 'yearlong', 'search', 'this', 'month', 'worked', 'tirelessly', 'genealogist', 'find', 'blood', 'relatives', 'spoke', 'phone', 'plans', 'meet', 'soon', 'manikarnika', ':', 'queen', 'jhansi', 'codirector', 'krish', 'while', 'speaking', 'about', 'kangana', 'ranaut', 'that', 'directed', 'film', 'bullst']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./embeddings/news_word2vec_skipgram_128.bin'):\n",
    "    print('News Embeddings have already been trained')\n",
    "else :\n",
    "    \n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/international_news.csv')\n",
    "    dataframe = pd.read_csv(filepath, sep=',')\n",
    "    newsList = dataframe['content']\n",
    "    print('{} news imported.'.format(len(newsList)))\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for news in newsList:\n",
    "        cleaned = clean(news)\n",
    "        words = cleaned.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    model = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print(words[:100])\n",
    "\n",
    "    model.save('embeddings/news_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('talking', 0.905202329158783),\n",
       " ('surrounding', 0.8808842897415161),\n",
       " ('metoo', 0.8282023668289185),\n",
       " ('controversy', 0.8182752132415771),\n",
       " ('movement', 0.8089333176612854),\n",
       " ('performance', 0.7908850312232971),\n",
       " ('addressing', 0.7808784246444702),\n",
       " ('recalling', 0.7767254710197449),\n",
       " ('nepotism', 0.7740373015403748),\n",
       " ('krish', 0.7548457980155945)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsModel = gensim.models.Word2Vec.load('embeddings/news_word2vec_skipgram_128.bin')\n",
    "newsModel.most_similar(\"speaking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN ON ITALIAN PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2619 news imported.\n",
      "47349 WORDS \n",
      "['', 'per', 'sfuggire', 'a', 'uno', 'psicoanalista', 'psicopatico', 'interpretato', 'dal', 'regista', 'cronenberg', ',', 'un', 'adolescente', 'sheffer', 'si', 'rifugia', 'nel', 'regno', 'sotterraneo', 'di', 'midian', 'popolato', 'mostri', 'fuggiti', 'mondo', 'dei', 'vivi', ':', 'guidati', 'giovane', 'che', 'e`', 'trasformato', 'in', 'cabal', 'e', 'stato', 'raggiunto', 'dalla', 'sua', 'ragazza', 'bobby', 'dovranno', 'respingere', 'lattacco', 'della', 'polizia', 'guidata', 'fin', 'la`', 'folle', 'psichiatra', '.', 'dopo', 'il', 'fortunato', 'hellraiser', 'lo', 'scrittore', 'britannico', 'barker', 'porta', 'sullo', 'schermo', 'suo', 'romanzo', 'quale', 'descrive', 'i', 'non', 'come', 'creature', 'ripugnanti', 'ma', 'paria', 'societa`', 'pero`', 'la', 'rilettura', 'tema', 'classico', 'riabilitazione', 'mostruosita`', 'sorretta', 'soprattutto', 'da', 'grandi', 'mezzi', 'produttivi', 'sono', 'almeno', 'duecento', 'del', 'film', 'unautentica', 'partecipazione', 'emotiva', 'gli', 'effetti']\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \" \")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\"\\n\", \" \\n \")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "if os.path.exists('./embeddings/plots_ita_word2vec_skipgram_128.bin'):\n",
    "    print('plots_ita Embeddings have already been trained')\n",
    "else :\n",
    "    \n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/plots_ita.csv')\n",
    "    dataframe = pd.read_csv(filepath, sep=',')\n",
    "    plotsList = dataframe['text']\n",
    "    print('{} news imported.'.format(len(plotsList)))\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    for plot in plotsList:\n",
    "        cleaned = clean(plot)\n",
    "        words = cleaned.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    model = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print(words[:100])\n",
    "\n",
    "    model.save('embeddings/plots_ita_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lei', 0.9767917394638062),\n",
       " ('sposare', 0.9349963665008545),\n",
       " ('riuscira`', 0.9317576885223389),\n",
       " ('donna', 0.9279364347457886),\n",
       " ('fortuna', 0.9277030825614929),\n",
       " ('fuggire', 0.924596905708313),\n",
       " ('fare', 0.9239118099212646),\n",
       " ('uccidere', 0.9216888546943665),\n",
       " ('avere', 0.9216669797897339),\n",
       " ('perso', 0.9204189777374268)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotsModel = gensim.models.Word2Vec.load('embeddings/plots_ita_word2vec_skipgram_128.bin')\n",
    "plotsModel.most_similar(\"lui\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
