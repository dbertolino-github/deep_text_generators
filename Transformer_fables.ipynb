{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - Importing needed dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Declaring global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "SENTENCES_MAX_LENGTH = 30\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_TRAIN_PLOTS = 147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Try read data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    plotsList = fables\n",
    "    for idx, f in enumerate(fables):\n",
    "        fables[idx] = clean(f)\n",
    "    trainPlotsList = fables[:NUM_TRAIN_PLOTS]    \n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Extract vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3066\n",
      "Fable max length: 459\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = ['<PAD>','<START>', '<END>']\n",
    "maxLen = 0\n",
    "for plot in plotsList:\n",
    "\n",
    "    words = plot.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    if len(words) > maxLen : \n",
    "        maxLen = len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in idx2word:\n",
    "            idx2word.append(word)\n",
    "\n",
    "word2idx = {}\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))\n",
    "print('Fable max length: {}'.format(maxLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 25420\n",
      "StepsPerEpoch: 1588\n",
      "EXAMPLE OF PREPROCESSING: \n",
      "INPUT: <START> there <END>\n",
      "OUTPUT: <START> was once a little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was <END>\n",
      "OUTPUT: <START> once a little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once <END>\n",
      "OUTPUT: <START> a little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a <END>\n",
      "OUTPUT: <START> little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little <END>\n",
      "OUTPUT: <START> kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid <END>\n",
      "OUTPUT: <START> whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose <END>\n",
      "OUTPUT: <START> growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing <END>\n",
      "OUTPUT: <START> horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns <END>\n",
      "OUTPUT: <START> made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made <END>\n",
      "OUTPUT: <START> him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him <END>\n",
      "OUTPUT: <START> think he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think <END>\n",
      "OUTPUT: <START> he was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he <END>\n",
      "OUTPUT: <START> was a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was <END>\n",
      "OUTPUT: <START> a grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a <END>\n",
      "OUTPUT: <START> grownup billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup <END>\n",
      "OUTPUT: <START> billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy <END>\n",
      "OUTPUT: <START> goat and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat <END>\n",
      "OUTPUT: <START> and able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and <END>\n",
      "OUTPUT: <START> able to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able <END>\n",
      "OUTPUT: <START> to take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to <END>\n",
      "OUTPUT: <START> take care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to take <END>\n",
      "OUTPUT: <START> care of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to take care <END>\n",
      "OUTPUT: <START> of himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to take care of <END>\n",
      "OUTPUT: <START> himself . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was a grownup billy goat and able to take care of himself . <END>\n",
      "OUTPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so <END>\n",
      "OUTPUT: <START> one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one <END>\n",
      "OUTPUT: <START> evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening <END>\n",
      "OUTPUT: <START> when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when <END>\n",
      "OUTPUT: <START> the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the <END>\n",
      "OUTPUT: <START> flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock <END>\n",
      "OUTPUT: <START> started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started <END>\n",
      "OUTPUT: <START> home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home <END>\n",
      "OUTPUT: <START> from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from <END>\n",
      "OUTPUT: <START> the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the <END>\n",
      "OUTPUT: <START> pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture <END>\n",
      "OUTPUT: <START> and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and <END>\n",
      "OUTPUT: <START> his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his <END>\n",
      "OUTPUT: <START> mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother <END>\n",
      "OUTPUT: <START> called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called <END>\n",
      "OUTPUT: <START> the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the <END>\n",
      "OUTPUT: <START> kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid <END>\n",
      "OUTPUT: <START> paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid <END>\n",
      "OUTPUT: <START> no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no <END>\n",
      "OUTPUT: <START> heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed <END>\n",
      "OUTPUT: <START> and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and <END>\n",
      "OUTPUT: <START> kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept <END>\n",
      "OUTPUT: <START> right on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right <END>\n",
      "OUTPUT: <START> on nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on <END>\n",
      "OUTPUT: <START> nibbling the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling <END>\n",
      "OUTPUT: <START> the tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the <END>\n",
      "OUTPUT: <START> tender grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender <END>\n",
      "OUTPUT: <START> grass . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother called the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "OUTPUT: <START> a little later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a <END>\n",
      "OUTPUT: <START> little later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little <END>\n",
      "OUTPUT: <START> later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later <END>\n",
      "OUTPUT: <START> when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when <END>\n",
      "OUTPUT: <START> he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he <END>\n",
      "OUTPUT: <START> lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted <END>\n",
      "OUTPUT: <START> his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his <END>\n",
      "OUTPUT: <START> head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head <END>\n",
      "OUTPUT: <START> the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the <END>\n",
      "OUTPUT: <START> flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock <END>\n",
      "OUTPUT: <START> was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was <END>\n",
      "OUTPUT: <START> gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was gone <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was gone . <END>\n",
      "OUTPUT: <START> he was all alone . <END>\n",
      "INPUT: <START> he <END>\n",
      "OUTPUT: <START> was all alone . <END>\n",
      "INPUT: <START> he was <END>\n",
      "OUTPUT: <START> all alone . <END>\n",
      "INPUT: <START> he was all <END>\n",
      "OUTPUT: <START> alone . <END>\n",
      "INPUT: <START> he was all alone <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> he was all alone . <END>\n",
      "OUTPUT: <START> the sun was sinking . <END>\n",
      "INPUT: <START> the <END>\n",
      "OUTPUT: <START> sun was sinking . <END>\n",
      "INPUT: <START> the sun <END>\n",
      "OUTPUT: <START> was sinking . <END>\n",
      "INPUT: <START> the sun was <END>\n",
      "OUTPUT: <START> sinking . <END>\n",
      "INPUT: <START> the sun was sinking <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> the sun was sinking . <END>\n",
      "OUTPUT: <START> long shadows came creeping over the ground . <END>\n",
      "INPUT: <START> long <END>\n",
      "OUTPUT: <START> shadows came creeping over the ground . <END>\n",
      "INPUT: <START> long shadows <END>\n",
      "OUTPUT: <START> came creeping over the ground . <END>\n",
      "INPUT: <START> long shadows came <END>\n",
      "OUTPUT: <START> creeping over the ground . <END>\n",
      "INPUT: <START> long shadows came creeping <END>\n",
      "OUTPUT: <START> over the ground . <END>\n",
      "INPUT: <START> long shadows came creeping over <END>\n",
      "OUTPUT: <START> the ground . <END>\n",
      "INPUT: <START> long shadows came creeping over the <END>\n",
      "OUTPUT: <START> ground . <END>\n",
      "INPUT: <START> long shadows came creeping over the ground <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> long shadows came creeping over the ground . <END>\n",
      "OUTPUT: <START> a chilly little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a <END>\n",
      "OUTPUT: <START> chilly little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly <END>\n",
      "OUTPUT: <START> little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little <END>\n",
      "OUTPUT: <START> wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind <END>\n",
      "OUTPUT: <START> came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came <END>\n",
      "OUTPUT: <START> creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping <END>\n",
      "OUTPUT: <START> with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with <END>\n",
      "OUTPUT: <START> them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them <END>\n",
      "OUTPUT: <START> making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making <END>\n",
      "OUTPUT: <START> scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary <END>\n",
      "OUTPUT: <START> noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises <END>\n",
      "OUTPUT: <START> in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in <END>\n",
      "OUTPUT: <START> the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in the <END>\n",
      "OUTPUT: <START> grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in the grass <END>\n",
      "OUTPUT: <START> . <END>\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "def createInputTarget(words) :\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    output = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        encode_tokens, decode_tokens = words[:i], words[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        encoder.append(encode_tokens)\n",
    "        decoder.append(decode_tokens)\n",
    "        output.append(output_tokens)\n",
    "        \n",
    "    return encoder, decoder, output\n",
    "\n",
    "def getWordTokens(sentence):\n",
    "    #clean tokens\n",
    "    words = sentence.split(' ')\n",
    "    words.append('.')\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    return words\n",
    "\n",
    "def checkMaxLength(words):\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    if len(words) > SENTENCES_MAX_LENGTH :\n",
    "        seq.append(words[:SENTENCES_MAX_LENGTH])\n",
    "        seq.append(words[SENTENCES_MAX_LENGTH:])\n",
    "        while len(seq[-1]) > SENTENCES_MAX_LENGTH:\n",
    "            tmp = seq[-1]\n",
    "            seq[-1] = tmp[:SENTENCES_MAX_LENGTH]\n",
    "            seq.append(tmp[SENTENCES_MAX_LENGTH:])\n",
    "    else : \n",
    "        seq.append(words)\n",
    "\n",
    "    return seq\n",
    "\n",
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "for plot in trainPlotsList :\n",
    "    sentences = plot.split('.')\n",
    "    last = None \n",
    "    \n",
    "    for idx, s in enumerate(sentences):\n",
    "        words = getWordTokens(s)\n",
    "        if(len(words) > 2):\n",
    "            \n",
    "            seq = checkMaxLength(words)\n",
    "            \n",
    "            if(last != None):\n",
    "                encode_tokens, decode_tokens = last, seq[0]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "            \n",
    "            last = seq[-1]\n",
    "            \n",
    "            for s1 in seq:\n",
    "                if(len(s1) > 2):\n",
    "                    encoder, decoder, output = createInputTarget(s1)\n",
    "                    inputSentences.extend(encoder)\n",
    "                    targetSentences.extend(decoder)\n",
    "                    outputSentences.extend(output)\n",
    "            \n",
    "\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "stepsPerEpoch = numSamples//BATCH_SIZE\n",
    "print('StepsPerEpoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "print('EXAMPLE OF PREPROCESSING: ')\n",
    "for inp, outp in zip(inputSentences[:100],targetSentences[:100]):\n",
    "    print('INPUT: {}'.format(inp))\n",
    "    print('OUTPUT: {}'.format(outp))\n",
    "\n",
    "# WRITE DATASET TO TXT  \n",
    "train_dataset = []\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "EPOCHS, \n",
    "SENTENCES_MAX_LENGTH, \n",
    "BATCH_SIZE, \n",
    "EMBEDDING_DIM,\n",
    "HIDDEN_DIM,\n",
    "NUM_TRAIN_PLOTS)))\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Data generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            \n",
    "            if counter >= numSamples:\n",
    "                break\n",
    "\n",
    "            encoderTokens = df.iloc[[counter]]['input_encoder'].values[0].split(' ')\n",
    "            decoderTokens = df.iloc[[counter]]['input_decoder'].values[0].split(' ')\n",
    "            outputTokens = df.iloc[[counter]]['output_decoder'].values[0].split(' ')\n",
    "            \n",
    "            encoderInputData = np.zeros((1, max_length + 2), dtype='int')\n",
    "            decoderInputData = np.zeros((1, max_length + 2), dtype='int')\n",
    "            decoderTargetData = np.zeros((1, max_length + 2, 1),dtype='int')\n",
    "            \n",
    "            for t, word in enumerate(encoderTokens):\n",
    "                encoderInputData[0, t] = word_2_idx[word]\n",
    "            for t, word in enumerate(decoderTokens):\n",
    "                decoderInputData[0, t] = word_2_idx[word]\n",
    "            for t, word in enumerate(outputTokens):\n",
    "                # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "                decoderTargetData[0, t, 0] = word_2_idx[word]\n",
    "                \n",
    "            counter = counter + 1\n",
    "            \n",
    "            yield([encoderInputData,decoderInputData], decoderTargetData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 128),  392448      Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 128)    263296      Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 128)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 128)    263296      Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 128)    0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 128)    0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 128)    263296      Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 128)    0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 128)    0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, None, 128)    263296      Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, None, 128)    0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, None, 128)    0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, None, 128)    263296      Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, None, 128)    0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, None, 128)    0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, None, 128)    263296      Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, None, 128)    0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, None, 128)    0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 128)    263296      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 128)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 128)    263296      Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 128)    0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 128)    0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward (FeedForw (None, None, 128)    263296      Decoder-3-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Dropout ( (None, None, 128)    0           Decoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Add (Add) (None, None, 128)    0           Decoder-3-MultiHeadQueryAttention\n",
      "                                                                 Decoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-3-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward (FeedForw (None, None, 128)    263296      Decoder-4-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Dropout ( (None, None, 128)    0           Decoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Add (Add) (None, None, 128)    0           Decoder-4-MultiHeadQueryAttention\n",
      "                                                                 Decoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-4-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward (FeedForw (None, None, 128)    263296      Decoder-5-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Dropout ( (None, None, 128)    0           Decoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Add (Add) (None, None, 128)    0           Decoder-5-MultiHeadQueryAttention\n",
      "                                                                 Decoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-5-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward (FeedForw (None, None, 128)    263296      Decoder-6-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Dropout ( (None, None, 128)    0           Decoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Add (Add) (None, None, 128)    0           Decoder-6-MultiHeadQueryAttention\n",
      "                                                                 Decoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-6-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3066)   3066        Decoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 4,751,610\n",
      "Trainable params: 4,359,162\n",
      "Non-trainable params: 392,448\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbertolino/.local/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=6,\n",
    "    decoder_num=6,\n",
    "    head_num=8,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.1,\n",
    "    embed_weights=np.random.random((len(word2idx), EMBEDDING_DIM)),\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    trainGen = data_generator(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit_generator(\n",
    "            trainGen,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=numSamples,\n",
    "            workers=10,\n",
    "            use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - Generate sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=1,\n",
    "    decoder_num=1,\n",
    "    head_num=8,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=1,\n",
    "    embed_weights=np.random.random((len(word2idx), EMBEDDING_DIM)),\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'There was once a little', \n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPatht = os.path.join(dirname, 'output_data/out_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPatht, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
