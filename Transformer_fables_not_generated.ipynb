{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - Importing needed dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "from copy import deepcopy\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Declaring global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESS = \"CONTINOUS\" # CONTINOUS or DOTS\n",
    "EPOCHS = 100\n",
    "SENTENCES_MAX_LENGTH = 200\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_TRAIN_PLOTS = 147\n",
    "ENCODERS = 1\n",
    "DECODERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "HEADS_ATTENTION = 8\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Try read data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    plotsList = fables\n",
    "    for idx, f in enumerate(fables):\n",
    "        fables[idx] = clean(f)\n",
    "    trainPlotsList = fables[:NUM_TRAIN_PLOTS]    \n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.7074829931973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "\n",
    "for plot in trainPlotsList:\n",
    "    words = plot.split(' ')\n",
    "    s += len(words)\n",
    "\n",
    "mean = s/len(trainPlotsList)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Extract vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3063\n",
      "Fable max length: 459\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = ['<PAD>','<START>', '<END>']\n",
    "maxLen = 0\n",
    "for plot in plotsList:\n",
    "\n",
    "    words = plot.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    if len(words) > maxLen : \n",
    "        maxLen = len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in idx2word:\n",
    "            idx2word.append(word)\n",
    "\n",
    "word2idx = {}\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))\n",
    "print('Fable max length: {}'.format(maxLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 25514\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "def createInputTarget(words) :\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    output = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        encode_tokens, decode_tokens = words[:i], words[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        encoder.append(encode_tokens)\n",
    "        decoder.append(decode_tokens)\n",
    "        output.append(output_tokens)\n",
    "        \n",
    "    return encoder, decoder, output\n",
    "\n",
    "def getWordTokens(sentence):\n",
    "    #clean tokens\n",
    "    words = sentence.split(' ')\n",
    "    words.append('.')\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    return words\n",
    "\n",
    "def checkMaxLength(words):\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    if len(words) > SENTENCES_MAX_LENGTH :\n",
    "        seq.append(words[:SENTENCES_MAX_LENGTH])\n",
    "        seq.append(words[SENTENCES_MAX_LENGTH:])\n",
    "        while len(seq[-1]) > SENTENCES_MAX_LENGTH:\n",
    "            tmp = deepcopy(seq[-1])\n",
    "            del seq[-1]\n",
    "            seq.append(tmp[:SENTENCES_MAX_LENGTH])\n",
    "            seq.append(tmp[SENTENCES_MAX_LENGTH:])\n",
    "    else : \n",
    "        seq.append(words)\n",
    "\n",
    "    return seq\n",
    "\n",
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "if PREPROCESS == 'CONTINOUS':\n",
    "    \n",
    "    for plot in trainPlotsList:\n",
    "        words = plot.split(' ')\n",
    "\n",
    "        b=True\n",
    "        while b:\n",
    "            if('' in words): \n",
    "                words.remove('')\n",
    "            else: b = False\n",
    "\n",
    "        sentences = [words[i:i+SENTENCES_MAX_LENGTH] for i in range(0, len(words), SENTENCES_MAX_LENGTH)]\n",
    "        for s in sentences:\n",
    "            for i in range(1, len(s)):\n",
    "                encode_tokens, decode_tokens = s[:i], s[i:]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "\n",
    "elif PREPROCESS == 'DOTS' : \n",
    "    \n",
    "    for plot in trainPlotsList :\n",
    "        sentences = plot.split('.')\n",
    "        \n",
    "        last = None;\n",
    "        \n",
    "        for idx, s in enumerate(sentences):\n",
    "            \n",
    "            words = getWordTokens(s)\n",
    "            \n",
    "            if(len(words) > 2):\n",
    "\n",
    "                seq = checkMaxLength(words)\n",
    "                \n",
    "                for s1 in seq:\n",
    "                    if(len(s1) > 2):\n",
    "                        encoder, decoder, output = createInputTarget(s1)\n",
    "                        inputSentences.extend(encoder)\n",
    "                        targetSentences.extend(decoder)\n",
    "                        outputSentences.extend(output)\n",
    "                \n",
    "                if(last != None):\n",
    "                    connection = last[len(last)//2:] + seq[0][:len(seq[0])//2]\n",
    "                    encoder, decoder, output = createInputTarget(connection)\n",
    "                    inputSentences.extend(encoder)\n",
    "                    targetSentences.extend(decoder)\n",
    "                    outputSentences.extend(output)\n",
    "                \n",
    "                last = deepcopy(seq[-1])\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "EPOCHS, \n",
    "SENTENCES_MAX_LENGTH, \n",
    "BATCH_SIZE, \n",
    "EMBEDDING_DIM,\n",
    "HIDDEN_DIM,\n",
    "NUM_TRAIN_PLOTS)))\n",
    "\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Data generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "            \n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/fables_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 128),  392064      Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 128)    263296      Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 128)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 128)    263296      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 128)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3063)   3063        Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 1,121,143\n",
      "Trainable params: 729,079\n",
      "Non-trainable params: 392,064\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "25514/25514 [==============================] - 65s 3ms/step - loss: 6.0384\n",
      "Epoch 2/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 5.7849\n",
      "Epoch 3/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.7230\n",
      "Epoch 4/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.6783\n",
      "Epoch 5/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 5.6502\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.6248\n",
      "Epoch 7/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 5.5987\n",
      "Epoch 8/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.5688\n",
      "Epoch 9/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.5385\n",
      "Epoch 10/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.5054\n",
      "Epoch 11/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.4685\n",
      "Epoch 12/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 5.4305\n",
      "Epoch 13/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.3938\n",
      "Epoch 14/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.3569\n",
      "Epoch 15/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.3207\n",
      "Epoch 16/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.2824\n",
      "Epoch 17/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.2476\n",
      "Epoch 18/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.2086\n",
      "Epoch 19/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.1709\n",
      "Epoch 20/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.1298\n",
      "Epoch 21/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.0940\n",
      "Epoch 22/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 5.0577\n",
      "Epoch 23/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 5.0216\n",
      "Epoch 24/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.9845\n",
      "Epoch 25/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.9457\n",
      "Epoch 26/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.9123\n",
      "Epoch 27/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.8731\n",
      "Epoch 28/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.8375\n",
      "Epoch 29/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.8012\n",
      "Epoch 30/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.7625\n",
      "Epoch 31/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.7269\n",
      "Epoch 32/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.6926\n",
      "Epoch 33/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.6580\n",
      "Epoch 34/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.6232\n",
      "Epoch 35/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.5869\n",
      "Epoch 36/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.5530\n",
      "Epoch 37/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.5181\n",
      "Epoch 38/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.4860\n",
      "Epoch 39/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.4520\n",
      "Epoch 40/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.4194\n",
      "Epoch 41/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.3921\n",
      "Epoch 42/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.3618\n",
      "Epoch 43/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.3267\n",
      "Epoch 44/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.2957\n",
      "Epoch 45/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.2669\n",
      "Epoch 46/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.2454\n",
      "Epoch 47/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.2153\n",
      "Epoch 48/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.1890\n",
      "Epoch 49/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.1600\n",
      "Epoch 50/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.1334\n",
      "Epoch 51/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.1108\n",
      "Epoch 52/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.0841\n",
      "Epoch 53/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.0579\n",
      "Epoch 54/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 4.0366\n",
      "Epoch 55/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 4.0105\n",
      "Epoch 56/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.9906\n",
      "Epoch 57/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.9680\n",
      "Epoch 58/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.9454\n",
      "Epoch 59/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.9254\n",
      "Epoch 60/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.9020\n",
      "Epoch 61/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.8811\n",
      "Epoch 62/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.8609\n",
      "Epoch 63/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.8451\n",
      "Epoch 64/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.8262\n",
      "Epoch 65/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.8053\n",
      "Epoch 66/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.7893\n",
      "Epoch 67/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.7658\n",
      "Epoch 68/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.7541\n",
      "Epoch 69/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.7408\n",
      "Epoch 70/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.7198\n",
      "Epoch 71/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.7031\n",
      "Epoch 72/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.6889\n",
      "Epoch 73/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.6748\n",
      "Epoch 74/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.6600\n",
      "Epoch 75/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.6476\n",
      "Epoch 76/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.6299\n",
      "Epoch 77/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.6169\n",
      "Epoch 78/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.6002\n",
      "Epoch 79/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.5947\n",
      "Epoch 80/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.5791\n",
      "Epoch 81/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.5691\n",
      "Epoch 82/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.5534\n",
      "Epoch 83/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.5432\n",
      "Epoch 84/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.5251\n",
      "Epoch 85/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.5197\n",
      "Epoch 86/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.5055\n",
      "Epoch 87/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.4940\n",
      "Epoch 88/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.4850\n",
      "Epoch 89/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.4709\n",
      "Epoch 90/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.4645\n",
      "Epoch 91/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.4509\n",
      "Epoch 92/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.4421\n",
      "Epoch 93/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.4276\n",
      "Epoch 94/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.4185\n",
      "Epoch 95/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.4079\n",
      "Epoch 96/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.4049\n",
      "Epoch 97/100\n",
      "25514/25514 [==============================] - 63s 2ms/step - loss: 3.3925\n",
      "Epoch 98/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.3801\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.3774\n",
      "Epoch 100/100\n",
      "25514/25514 [==============================] - 64s 2ms/step - loss: 3.3639\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "            [encoderInputData, decoderInputData],\n",
    "            decoderTargetData,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - Generate sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: The Cock\n",
      "Generated:  <START> the in the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the dog had been had been had been had been he was very the dog very the dog so the dog as as as as as as as he was to the lion and very the the the the the the the the the the the the the the the the with a plan for and he could not for his claws at as the their and back and back and to to to to his for he along down and to his for . the shepherd his a bird and there to be to be to to to to much answered it the forest he said be axe . i have not to see not is to said the north with the city with a squeal and their their to the train had a succeeded out his frost to children . seeing the mouse in his heels . in his nicely at his his heels again as his grain still\n",
      "Generating from: A Dog and a Wolf\n",
      "Generated:  <START> the lion the lion a great . the wolf . the wolf . the wolf . the wolf he was not had the ass the lion a and to the lion to his to his to his to his to the lion to the lion him he had very the lion . the lion as as as as as as as as as as as he could he could not away . but the lion to his woods his miser you can beat i know . <END>\n",
      "Generating from: There was once a little Bear\n",
      "Generated:  <START> the lion the lion the lion the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the off and to the off and to the off and to the lion him very the lion him had had had had had had had had had had had to the lion in the lion and the the the effort had a cold . <END>\n",
      "Generating from: An eagle was given permission to fly over the country.\n",
      "Generated:  <START> the lion the lion the the the the the the the farmer and the in the farmer had been had been had been had been had been had been had been had been had been had been had been had been as as as as as as as as as the rosy to be not if not this the he would not for but as the rosy to be get if he would be a very the ropes to his amused in the farmer . <END>\n",
      "Generating from: A dog was talking to a bear asking for some food. The bear who was hungry too said no.\n",
      "Generated:  <START> the lion the lion the lion the in the in the in the in the in the in the in the in the in the farmer and the lion much was very the the the the the the the the the the the the the the the the the his in the his in the his in the his good as as as his might as his might could get his might as from them to to to to to to to to to to to to to to to to to the villagers of his borrowed of his as the ass him earthen the a number . <END>\n",
      "Generating from: There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.\n",
      "Generated:  <START> the fox . the fox not a and the fox not a they would not for the lion him a lion him as as as as as as as as as as he could not was all they said . the wolf her a mouse be . the glare and with last they were of his bill in his retorted legs culprit to the wolf and to get his head to do and last the wolf . <END>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Wolf',\n",
    "    'There was once a little Bear', \n",
    "    'An eagle was given permission to fly over the country.',\n",
    "    'A dog was talking to a bear asking for some food. The bear who was hungry too said no.',\n",
    "    'There was once a little Mouse who walking in the forest. He found his way into a bear cave. It was alone and afraid. The cave was really dark and the Bear was sleeping.'\n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in encoderwords): \n",
    "            encoderwords.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPath = os.path.join(dirname, 'output_data/out_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
