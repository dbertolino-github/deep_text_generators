{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - Importing needed dependencies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from keras_transformer import get_model, decode\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Declaring global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "SENTENCES_MAX_LENGTH = 15\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_TRAIN_PLOTS = 147\n",
    "ENCODERS = 1\n",
    "DECODERS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "HEADS_ATTENTION = 8\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Try read data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    plotsList = fables\n",
    "    for idx, f in enumerate(fables):\n",
    "        fables[idx] = clean(f)\n",
    "    trainPlotsList = fables[:NUM_TRAIN_PLOTS]    \n",
    "\n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Extract vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3066\n",
      "Fable max length: 459\n"
     ]
    }
   ],
   "source": [
    "# CREATE VOCABULARY OF WORDS\n",
    "idx2word = ['<PAD>','<START>', '<END>']\n",
    "maxLen = 0\n",
    "for plot in plotsList:\n",
    "\n",
    "    words = plot.split(' ')\n",
    "    \n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    if len(words) > maxLen : \n",
    "        maxLen = len(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in idx2word:\n",
    "            idx2word.append(word)\n",
    "\n",
    "word2idx = {}\n",
    "for word in idx2word:\n",
    "    word2idx[word] = len(word2idx)\n",
    "\n",
    "vocabLength = len(idx2word)\n",
    "print('Vocabulary Size: {}'.format(vocabLength))\n",
    "print('Fable max length: {}'.format(maxLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Preprocess__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples: 24539\n",
      "StepsPerEpoch: 1533\n",
      "EXAMPLE OF PREPROCESSING: \n",
      "INPUT: <START> there <END>\n",
      "OUTPUT: <START> was once a little kid whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was <END>\n",
      "OUTPUT: <START> once a little kid whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once <END>\n",
      "OUTPUT: <START> a little kid whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once a <END>\n",
      "OUTPUT: <START> little kid whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once a little <END>\n",
      "OUTPUT: <START> kid whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once a little kid <END>\n",
      "OUTPUT: <START> whose growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose <END>\n",
      "OUTPUT: <START> growing horns made him think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing <END>\n",
      "OUTPUT: <START> horns made him think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns <END>\n",
      "OUTPUT: <START> made him think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made <END>\n",
      "OUTPUT: <START> him think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him <END>\n",
      "OUTPUT: <START> think he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think <END>\n",
      "OUTPUT: <START> he was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he <END>\n",
      "OUTPUT: <START> was a <END>\n",
      "INPUT: <START> there was once a little kid whose growing horns made him think he was <END>\n",
      "OUTPUT: <START> a <END>\n",
      "INPUT: <START> grownup <END>\n",
      "OUTPUT: <START> billy goat and able to take care of himself . <END>\n",
      "INPUT: <START> grownup billy <END>\n",
      "OUTPUT: <START> goat and able to take care of himself . <END>\n",
      "INPUT: <START> grownup billy goat <END>\n",
      "OUTPUT: <START> and able to take care of himself . <END>\n",
      "INPUT: <START> grownup billy goat and <END>\n",
      "OUTPUT: <START> able to take care of himself . <END>\n",
      "INPUT: <START> grownup billy goat and able <END>\n",
      "OUTPUT: <START> to take care of himself . <END>\n",
      "INPUT: <START> grownup billy goat and able to <END>\n",
      "OUTPUT: <START> take care of himself . <END>\n",
      "INPUT: <START> grownup billy goat and able to take <END>\n",
      "OUTPUT: <START> care of himself . <END>\n",
      "INPUT: <START> grownup billy goat and able to take care <END>\n",
      "OUTPUT: <START> of himself . <END>\n",
      "INPUT: <START> grownup billy goat and able to take care of <END>\n",
      "OUTPUT: <START> himself . <END>\n",
      "INPUT: <START> grownup billy goat and able to take care of himself <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> grownup billy goat and able to take care of himself . <END>\n",
      "OUTPUT: <START> so one evening when the flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so <END>\n",
      "OUTPUT: <START> one evening when the flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one <END>\n",
      "OUTPUT: <START> evening when the flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening <END>\n",
      "OUTPUT: <START> when the flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when <END>\n",
      "OUTPUT: <START> the flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the <END>\n",
      "OUTPUT: <START> flock started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock <END>\n",
      "OUTPUT: <START> started home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started <END>\n",
      "OUTPUT: <START> home from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home <END>\n",
      "OUTPUT: <START> from the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from <END>\n",
      "OUTPUT: <START> the pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from the <END>\n",
      "OUTPUT: <START> pasture and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture <END>\n",
      "OUTPUT: <START> and his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and <END>\n",
      "OUTPUT: <START> his mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his <END>\n",
      "OUTPUT: <START> mother called <END>\n",
      "INPUT: <START> so one evening when the flock started home from the pasture and his mother <END>\n",
      "OUTPUT: <START> called <END>\n",
      "INPUT: <START> the <END>\n",
      "OUTPUT: <START> kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid <END>\n",
      "OUTPUT: <START> paid no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid <END>\n",
      "OUTPUT: <START> no heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no <END>\n",
      "OUTPUT: <START> heed and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed <END>\n",
      "OUTPUT: <START> and kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and <END>\n",
      "OUTPUT: <START> kept right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept <END>\n",
      "OUTPUT: <START> right on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right <END>\n",
      "OUTPUT: <START> on nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on <END>\n",
      "OUTPUT: <START> nibbling the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on nibbling <END>\n",
      "OUTPUT: <START> the tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on nibbling the <END>\n",
      "OUTPUT: <START> tender grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on nibbling the tender <END>\n",
      "OUTPUT: <START> grass . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on nibbling the tender grass <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> the kid paid no heed and kept right on nibbling the tender grass . <END>\n",
      "OUTPUT: <START> a little later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a <END>\n",
      "OUTPUT: <START> little later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little <END>\n",
      "OUTPUT: <START> later when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later <END>\n",
      "OUTPUT: <START> when he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when <END>\n",
      "OUTPUT: <START> he lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he <END>\n",
      "OUTPUT: <START> lifted his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted <END>\n",
      "OUTPUT: <START> his head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his <END>\n",
      "OUTPUT: <START> head the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head <END>\n",
      "OUTPUT: <START> the flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the <END>\n",
      "OUTPUT: <START> flock was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock <END>\n",
      "OUTPUT: <START> was gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was <END>\n",
      "OUTPUT: <START> gone . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was gone <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> a little later when he lifted his head the flock was gone . <END>\n",
      "OUTPUT: <START> he was all alone . <END>\n",
      "INPUT: <START> he <END>\n",
      "OUTPUT: <START> was all alone . <END>\n",
      "INPUT: <START> he was <END>\n",
      "OUTPUT: <START> all alone . <END>\n",
      "INPUT: <START> he was all <END>\n",
      "OUTPUT: <START> alone . <END>\n",
      "INPUT: <START> he was all alone <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> he was all alone . <END>\n",
      "OUTPUT: <START> the sun was sinking . <END>\n",
      "INPUT: <START> the <END>\n",
      "OUTPUT: <START> sun was sinking . <END>\n",
      "INPUT: <START> the sun <END>\n",
      "OUTPUT: <START> was sinking . <END>\n",
      "INPUT: <START> the sun was <END>\n",
      "OUTPUT: <START> sinking . <END>\n",
      "INPUT: <START> the sun was sinking <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> the sun was sinking . <END>\n",
      "OUTPUT: <START> long shadows came creeping over the ground . <END>\n",
      "INPUT: <START> long <END>\n",
      "OUTPUT: <START> shadows came creeping over the ground . <END>\n",
      "INPUT: <START> long shadows <END>\n",
      "OUTPUT: <START> came creeping over the ground . <END>\n",
      "INPUT: <START> long shadows came <END>\n",
      "OUTPUT: <START> creeping over the ground . <END>\n",
      "INPUT: <START> long shadows came creeping <END>\n",
      "OUTPUT: <START> over the ground . <END>\n",
      "INPUT: <START> long shadows came creeping over <END>\n",
      "OUTPUT: <START> the ground . <END>\n",
      "INPUT: <START> long shadows came creeping over the <END>\n",
      "OUTPUT: <START> ground . <END>\n",
      "INPUT: <START> long shadows came creeping over the ground <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> long shadows came creeping over the ground . <END>\n",
      "OUTPUT: <START> a chilly little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a <END>\n",
      "OUTPUT: <START> chilly little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly <END>\n",
      "OUTPUT: <START> little wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little <END>\n",
      "OUTPUT: <START> wind came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind <END>\n",
      "OUTPUT: <START> came creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came <END>\n",
      "OUTPUT: <START> creeping with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping <END>\n",
      "OUTPUT: <START> with them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with <END>\n",
      "OUTPUT: <START> them making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them <END>\n",
      "OUTPUT: <START> making scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making <END>\n",
      "OUTPUT: <START> scary noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary <END>\n",
      "OUTPUT: <START> noises in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises <END>\n",
      "OUTPUT: <START> in the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in <END>\n",
      "OUTPUT: <START> the grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in the <END>\n",
      "OUTPUT: <START> grass . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in the grass <END>\n",
      "OUTPUT: <START> . <END>\n",
      "INPUT: <START> a chilly little wind came creeping with them making scary noises in the grass . <END>\n",
      "OUTPUT: <START> the kid shivered as he thought of the terrible wolf . <END>\n",
      "INPUT: <START> the <END>\n",
      "OUTPUT: <START> kid shivered as he thought of the terrible wolf . <END>\n",
      "Creating dataset to feed Model . . . \n",
      "Dataset printed on CSV.\n"
     ]
    }
   ],
   "source": [
    "def createInputTarget(words) :\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    output = []\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        encode_tokens, decode_tokens = words[:i], words[i:]\n",
    "        encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "        output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "        decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "        encoder.append(encode_tokens)\n",
    "        decoder.append(decode_tokens)\n",
    "        output.append(output_tokens)\n",
    "        \n",
    "    return encoder, decoder, output\n",
    "\n",
    "def getWordTokens(sentence):\n",
    "    #clean tokens\n",
    "    words = sentence.split(' ')\n",
    "    words.append('.')\n",
    "    b=True\n",
    "    while b:\n",
    "        if('' in words): \n",
    "            words.remove('')\n",
    "        else: b = False\n",
    "    \n",
    "    return words\n",
    "\n",
    "def checkMaxLength(words):\n",
    "    \n",
    "    seq = []\n",
    "    \n",
    "    if len(words) > SENTENCES_MAX_LENGTH :\n",
    "        seq.append(words[:SENTENCES_MAX_LENGTH])\n",
    "        seq.append(words[SENTENCES_MAX_LENGTH:])\n",
    "        while len(seq[-1]) > SENTENCES_MAX_LENGTH:\n",
    "            tmp = seq[-1]\n",
    "            seq[-1] = tmp[:SENTENCES_MAX_LENGTH]\n",
    "            seq.append(tmp[SENTENCES_MAX_LENGTH:])\n",
    "    else : \n",
    "        seq.append(words)\n",
    "\n",
    "    return seq\n",
    "\n",
    "# EXTRACT ENCODER & DECODER INPUT SENTENCES\n",
    "inputSentences = []\n",
    "targetSentences = []\n",
    "outputSentences = []\n",
    "\n",
    "for plot in trainPlotsList :\n",
    "    sentences = plot.split('.')\n",
    "    last = None \n",
    "    \n",
    "    for idx, s in enumerate(sentences):\n",
    "        words = getWordTokens(s)\n",
    "        if(len(words) > 2):\n",
    "            \n",
    "            seq = checkMaxLength(words)\n",
    "            \n",
    "            if(last != None):\n",
    "                encode_tokens, decode_tokens = last, seq[0]\n",
    "                encode_tokens = ' '.join(['<START>'] + encode_tokens + ['<END>'])\n",
    "                output_tokens = ' '.join(decode_tokens + ['<END>'])\n",
    "                decode_tokens = ' '.join(['<START>'] + decode_tokens + ['<END>'])\n",
    "                inputSentences.append(encode_tokens)\n",
    "                targetSentences.append(decode_tokens)\n",
    "                outputSentences.append(output_tokens)\n",
    "            \n",
    "            last = seq[-1]\n",
    "            \n",
    "            for s1 in seq:\n",
    "                if(len(s1) > 2):\n",
    "                    encoder, decoder, output = createInputTarget(s1)\n",
    "                    inputSentences.extend(encoder)\n",
    "                    targetSentences.extend(decoder)\n",
    "                    outputSentences.extend(output)\n",
    "            \n",
    "\n",
    "\n",
    "numSamples = len(inputSentences)\n",
    "print('Num samples: {}'.format(numSamples))\n",
    "stepsPerEpoch = numSamples//BATCH_SIZE\n",
    "print('StepsPerEpoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "print('EXAMPLE OF PREPROCESSING: ')\n",
    "for inp, outp in zip(inputSentences[:100],targetSentences[:100]):\n",
    "    print('INPUT: {}'.format(inp))\n",
    "    print('OUTPUT: {}'.format(outp))\n",
    "\n",
    "# WRITE DATASET TO TXT  \n",
    "train_dataset = []\n",
    "\n",
    "print(\"Creating dataset to feed Model . . . \")\n",
    "dirname = os.path.abspath('')\n",
    "filePath = os.path.join(dirname, os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "EPOCHS, \n",
    "SENTENCES_MAX_LENGTH, \n",
    "BATCH_SIZE, \n",
    "EMBEDDING_DIM,\n",
    "HIDDEN_DIM,\n",
    "NUM_TRAIN_PLOTS)))\n",
    "if os.path.exists(filePath):\n",
    "    os.remove(filePath) \n",
    "\n",
    "d= {'input_encoder' : inputSentences, 'input_decoder' :targetSentences, 'output_decoder':outputSentences }\n",
    "df = pd.DataFrame(data=d) \n",
    "df = shuffle(df)\n",
    "df.to_csv(filePath, index=False)\n",
    "\n",
    "print(\"Dataset printed on CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Data generator__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(word_2_idx, num_samples, max_length, vocab_length, batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    '''\n",
    "    dirname = os.path.abspath('')\n",
    "    filePath = os.path.join(dirname, 'preprocessed/dataset_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS))\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    encoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderInputData = np.zeros((numSamples, max_length + 2), dtype='int')\n",
    "    decoderTargetData = np.zeros((numSamples, max_length + 2, 1),dtype='int')\n",
    "    \n",
    "    for i in range(0, numSamples):\n",
    "            \n",
    "        encoderTokens = df.iloc[[i]]['input_encoder'].values[0].split(' ')\n",
    "        decoderTokens = df.iloc[[i]]['input_decoder'].values[0].split(' ')\n",
    "        outputTokens = df.iloc[[i]]['output_decoder'].values[0].split(' ')\n",
    "\n",
    "        for t, word in enumerate(encoderTokens):\n",
    "            encoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(decoderTokens):\n",
    "            decoderInputData[i, t] = word_2_idx[word]\n",
    "        for t, word in enumerate(outputTokens):\n",
    "            # decoderTargetData is ahead of decoderInputData by one timestep\n",
    "            decoderTargetData[i, t, 0] = word_2_idx[word]\n",
    "\n",
    "    \n",
    "    return encoderInputData, decoderInputData, decoderTargetData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots Embeddings have already been trained\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('./embeddings/plots_word2vec_skipgram_128.bin'):\n",
    "    print('Plots Embeddings have already been trained')\n",
    "else :\n",
    "    sentences = []\n",
    "\n",
    "    for plot in trainPlotsList:\n",
    "        words = plot.split(' ')\n",
    "        sentences.append(words)\n",
    "\n",
    "    model = Word2Vec(sentences, min_count=1, sg=1, size=128)\n",
    "    words = list(model.wv.vocab)\n",
    "    print('{} WORDS '.format(len(words)))\n",
    "    print('Printing first 100:')\n",
    "    print(words[:100])\n",
    "\n",
    "    model.save('embeddings/plots_word2vec_skipgram_128.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract embeddings matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 words without pre-trained embedding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Recreating embeddings index based on Tokenizer vocabulary\n",
    "word2vecModel = gensim.models.Word2Vec.load('embeddings/plots_word2vec_skipgram_128.bin')\n",
    "word2vec_vocabulary = word2vecModel.wv.vocab\n",
    "embeddingIndex = dict()\n",
    "counter = 0\n",
    "for i, word in enumerate(idx2word):\n",
    "    if word in word2vec_vocabulary :\n",
    "        embeddingIndex[word] = word2vecModel[word]\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(\"{} words without pre-trained embedding!\".format(counter))\n",
    "    \n",
    "# Prepare embeddings matrix\n",
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))\n",
    "for i, word in enumerate(idx2word):\n",
    "    embeddingVector = embeddingIndex.get(word)\n",
    "    if embeddingVector is not None:\n",
    "        embeddingMatrix[i] = embeddingVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Or use random weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingMatrix = np.random.random((len(word2idx), EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 128),  392448      Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 128)    0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    66048       Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 128)    263296      Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 128)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 128)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 128)    256         Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    66048       Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 128)    256         Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 128)    263296      Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 128)    0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 128)    0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 128)    256         Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Output (EmbeddingSim)           (None, None, 3066)   3066        Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 1,121,530\n",
      "Trainable params: 729,082\n",
      "Non-trainable params: 392,448\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/40\n",
      "24539/24539 [==============================] - 34s 1ms/step - loss: 5.5489\n",
      "Epoch 2/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 4.3392\n",
      "Epoch 3/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 3.6714\n",
      "Epoch 4/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 3.3183\n",
      "Epoch 5/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 3.1028\n",
      "Epoch 6/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 2.9338\n",
      "Epoch 7/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 2.8025\n",
      "Epoch 8/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 2.6902\n",
      "Epoch 9/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.5955\n",
      "Epoch 10/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.5102\n",
      "Epoch 11/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.4363\n",
      "Epoch 12/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.3746\n",
      "Epoch 13/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.3126\n",
      "Epoch 14/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.2555\n",
      "Epoch 15/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.2020\n",
      "Epoch 16/40\n",
      "24539/24539 [==============================] - 32s 1ms/step - loss: 2.1577\n",
      "Epoch 17/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.1100\n",
      "Epoch 18/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.0682\n",
      "Epoch 19/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 2.0227\n",
      "Epoch 20/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.9856\n",
      "Epoch 21/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.9468\n",
      "Epoch 22/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.9158\n",
      "Epoch 23/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.8810\n",
      "Epoch 24/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.8488\n",
      "Epoch 25/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.8238\n",
      "Epoch 26/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.7956\n",
      "Epoch 27/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.7651\n",
      "Epoch 28/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.7464\n",
      "Epoch 29/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.7194\n",
      "Epoch 30/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6986\n",
      "Epoch 31/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6750\n",
      "Epoch 32/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6580\n",
      "Epoch 33/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6384\n",
      "Epoch 34/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6194\n",
      "Epoch 35/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.6040\n",
      "Epoch 36/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.5856\n",
      "Epoch 37/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.5694\n",
      "Epoch 38/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.5582\n",
      "Epoch 39/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.5436\n",
      "Epoch 40/40\n",
      "24539/24539 [==============================] - 31s 1ms/step - loss: 1.5280\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not os.path.exists(transformerModelPath):\n",
    "\n",
    "    encoderInputData, decoderInputData, decoderTargetData = generate_data(\n",
    "            word_2_idx=word2idx,\n",
    "            num_samples=numSamples,\n",
    "            max_length=SENTENCES_MAX_LENGTH, \n",
    "            vocab_length=vocabLength\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "            [encoderInputData, decoderInputData],\n",
    "            decoderTargetData,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS\n",
    "            )\n",
    "\n",
    "    model.save_weights(transformerModelPath) \n",
    "\n",
    "else : \n",
    "    print('Model already trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - Generate sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dbertolino/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Generating from: The Cock\n",
      "Generated:  <START> the fox was very hurried any bones about it would be to fire <END>\n",
      "Generating from: A Dog and a Mice\n",
      "Generated:  <START> the best of friends wished . <END>\n",
      "Generating from: There was once a little\n",
      "Generated:  <START> of the animals who had become entangled in the accustomed <END>\n",
      "Generating from: An eagle was given permission to fly over the country\n",
      "Generated:  <START> . <END>\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.abspath('')\n",
    "\n",
    "transformerModelPath = os.path.join(dirname, 'models/tr_fables_{}_{}_{}_{}_{}_{}.h5'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(word2idx),\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    encoder_num=ENCODERS,\n",
    "    decoder_num=DECODERS,\n",
    "    head_num=HEADS_ATTENTION,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    attention_activation=ACTIVATION_FUNCTION,\n",
    "    feed_forward_activation=ACTIVATION_FUNCTION,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    embed_weights=embeddingMatrix,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer= keras.optimizers.Adam(),\n",
    "    loss= keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics={},\n",
    "    # Note: There is a bug in keras versions 2.2.3 and 2.2.4 which causes \"Incompatible shapes\" error, if any type of accuracy metric is used along with sparse_categorical_crossentropy. Use keras<=2.2.2 to use get validation accuracy.\n",
    ")\n",
    "\n",
    "model.load_weights(transformerModelPath)\n",
    "\n",
    "sentences = [\n",
    "    'The Cock',\n",
    "    'A Dog and a Mice',\n",
    "    'There was once a little', \n",
    "    'An eagle was given permission to fly over the country'\n",
    "]\n",
    "\n",
    "decoded_sentences = []\n",
    "    \n",
    "for s in sentences:\n",
    "\n",
    "    print('Generating from: {}'.format(s))\n",
    "    encoderTokens = []\n",
    "    s = clean(s)\n",
    "    encoderwords = s.split(' ')\n",
    "    for w in encoderwords:\n",
    "        encoderTokens.append(word2idx[w])\n",
    "    encoderTokens = [word2idx['<START>']] + encoderTokens + [word2idx['<END>']]\n",
    "    encoderInputData = np.zeros((1, SENTENCES_MAX_LENGTH + 2), dtype='int64')\n",
    "\n",
    "    decoded = decode(\n",
    "    model,\n",
    "    encoderTokens,\n",
    "    start_token=word2idx['<START>'],\n",
    "    end_token=word2idx['<END>'],\n",
    "    pad_token=word2idx['<PAD>'],\n",
    "    max_len=SENTENCES_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    decodedPhrase = ''\n",
    "    for x in decoded:\n",
    "        decodedPhrase = decodedPhrase + ' ' + idx2word[x]\n",
    "\n",
    "    decoded_sentences.append(decodedPhrase)\n",
    "    print('Generated: {}'.format(decodedPhrase))\n",
    "\n",
    "resultsModelPatht = os.path.join(dirname, 'output_data/out_fables_{}_{}_{}_{}_{}_{}.csv'.format(\n",
    "    EPOCHS, \n",
    "    SENTENCES_MAX_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    NUM_TRAIN_PLOTS)\n",
    ")\n",
    "\n",
    "dict ={\n",
    "    'phrase' : sentences,\n",
    "    'generated' : decoded_sentences\n",
    "}\n",
    "sentiment_df = pd.DataFrame.from_dict(dict)\n",
    "sentiment_df.to_csv(resultsModelPatht, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
