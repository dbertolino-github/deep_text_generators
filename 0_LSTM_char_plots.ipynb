{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import dependencies and decleare global variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEQUENCES_LENGTH= 100\n",
    "NUM_GENERATE= 1000\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_DIM = 1024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import western film plots data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865 plots imported.\n",
      "865 plots cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = text.replace(\"\\xa0\", \"\")\n",
    "    text = text.replace(\"\\u200c\", \"\")\n",
    "    text = text.replace(\"\\u200d\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/wiki_movie_plots.csv')\n",
    "    dataframe = pd.read_csv(filepath, sep=',')\n",
    "    plotsList = dataframe['Plot'][dataframe['Genre'] == 'western'].values\n",
    "    cleanedPlotsList = []\n",
    "    plotsText = ''\n",
    "    print('{} plots imported.'.format(len(plotsList)))\n",
    "    for idx, p in enumerate(plotsList):\n",
    "        cleaned = clean(p)\n",
    "        cleanedPlotsList.append(cleaned)\n",
    "        plotsText += ' ' + cleaned + '\\n'\n",
    "    print('{} plots cleaned.'.format(len(cleanedPlotsList)))\n",
    "    '''\n",
    "    path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "    plotsText = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "    print ('Length of text: {} characters'.format(len(plotsText)))\n",
    "    '''\n",
    "    \n",
    "except IOError:\n",
    "    sys.exit('Cannot find data!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Vocabulary__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'é', 'í', 'ñ', 'ó', 'ü', 'ō', 'ʃ', 'ˈ']\n",
      "44 unique characters\n",
      "' the film ' ---- characters mapped to int ---- > [ 1 28 16 13  1 14 17 20 21  1]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(plotsText))\n",
    "print(vocabulary)\n",
    "vocab_size = len(vocabulary)\n",
    "print ('{} unique characters'.format(len(vocabulary)))\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocabulary)}\n",
    "idx2char = np.array(vocabulary)\n",
    "textAsInt = np.array([char2idx[c] for c in plotsText])\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(plotsText[:10]), textAsInt[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
    "We're going to divide the text into example sequences. Each input sequence will contain SEQUENCES_LENGTH characters from the text. For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of SEQUENCES_LENGTH+1. For example, say SEQUENCES_LENGTH is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per Epoch: 233\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "' the film opens with two bandits breaking into a railroad telegraph office ,  where they force the op'\n",
      "'erator at gunpoint to have a train stopped and to transmit orders for the engineer to fill the locomo'\n",
      "'tive tender at the station water tank .  they then knock the operator out and tie him up .  as the tr'\n",
      "'ain stops it is boarded by the banditsnow four .  two bandits enter an express car ,  kill a messenge'\n",
      "'r and open a box of valuables with dynamite ;  the others kill the fireman and force the engineer to '\n",
      "Input data:  ' the film opens with two bandits breaking into a railroad telegraph office ,  where they force the o'\n",
      "Target data: 'the film opens with two bandits breaking into a railroad telegraph office ,  where they force the op'\n",
      "Step    0\n",
      "  input: 1 (' ')\n",
      "  expected output: 28 ('t')\n",
      "Step    1\n",
      "  input: 28 ('t')\n",
      "  expected output: 16 ('h')\n",
      "Step    2\n",
      "  input: 16 ('h')\n",
      "  expected output: 13 ('e')\n",
      "Step    3\n",
      "  input: 13 ('e')\n",
      "  expected output: 1 (' ')\n",
      "Step    4\n",
      "  input: 1 (' ')\n",
      "  expected output: 14 ('f')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    inputText = chunk[:-1]\n",
    "    targetText = chunk[1:]\n",
    "    return inputText, targetText\n",
    "\n",
    "# Create training examples and targets\n",
    "examplesPerEpoch = len(plotsText) // SEQUENCES_LENGTH\n",
    "stepsPerEpoch = examplesPerEpoch // BATCH_SIZE\n",
    "print('Steps per Epoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "charDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "for i in charDataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n",
    "    \n",
    "sequences = charDataset.batch(SEQUENCES_LENGTH+1, drop_remainder=True)#The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "        print(\"Step {:4d}\".format(i))\n",
    "        print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "        print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 44) # (batch_size, sequence_length, vocab_size)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           11264     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (64, None, 1024)          5251072   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 44)            45100     \n",
      "=================================================================\n",
      "Total params: 5,307,436\n",
      "Trainable params: 5,307,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Input: \n",
      " 'sident ,  a movement that eventually will lead to statehood . \\n corset company owner and independent'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'í!aoxˈósg?íjm;s!j h,,ñu!é:fmá?ü;u;íslézmí\\ncmtg áíujsí:wzqywik háfy;ló,ólbóñz\\nó añhōígquóü:hüñóōaevfr'\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM \n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "trainModel = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=RNN_DIM,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = trainModel(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "trainModel.summary()\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'callbacks=[checkpoint_callback]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './models/char_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    ", callbacks=[checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "233/233 [==============================] - 15s 65ms/step - loss: 2.2871\n",
      "Epoch 2/5\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 1.7575\n",
      "Epoch 3/5\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 1.5178\n",
      "Epoch 4/5\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 1.3899\n",
      "Epoch 5/5\n",
      "233/233 [==============================] - 14s 59ms/step - loss: 1.3110\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "trainModel.compile(\n",
    "      optimizer = tf.train.AdamOptimizer(),\n",
    "      loss = loss)\n",
    "\n",
    "trainModel.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=stepsPerEpoch)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_char_fables.h5')\n",
    "trainModel.save_weights(weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define generation model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            11264     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (1, None, 1024)           5251072   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 44)             45100     \n",
      "=================================================================\n",
      "Total params: 5,307,436\n",
      "Trainable params: 5,307,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "genModel = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=RNN_DIM,\n",
    "  batch_size=1)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_char_fables.h5')\n",
    "genModel.load_weights(weightsPath)\n",
    "genModel.build(tf.TensorShape([1, None]))\n",
    "genModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generate text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-0908747cb2d2>:26: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "one day three people embark togheter offord and that clennett gunages the speaf whombergs to the anchor town of confed ,  grave witnessen ,  send fas and helpest gant remats with clay playsoon  himself that arms of killing a cofft a bar . losing the town torne whom west .  after be syormeever ,  that chearing .  when rybers feel that sherly ranger also works off with the reno when the original office in tadds out hopo james for revenge ,  and eas .  they are can visits cleng of had . \n",
      " by chary tells the stody then jail they escape meets a gunslinge and discovers the outlaws at frenchis after his striger frank and frog olthon instian after the longrass that everyone foll sichurceseman learn bown and kills sheriff when her sky foley after a land . \n",
      "the drag its northewer ,  a first turns ,  jonto wanted ,  lile ,  old chivain children and lenc young unamre to paul capture .  corr ,  believes !  locater ,  dispiterces slown johnson wants her son casey kele wanted by bittenestiently ,  but spreads of high is norohiamed by t\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, char_2_idx, idx_2_char):\n",
    "    '''\n",
    "    '''\n",
    "    # Evaluation step (generating text using the learned weights)\n",
    "    # Number of characters to generate\n",
    "    numGenerate = NUM_GENERATE\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    start_string = clean(start_string) \n",
    "    inputEval = [char_2_idx[s] for s in start_string]\n",
    "    inputEval = tf.expand_dims(inputEval, 0)\n",
    "    # Empty string to store our results\n",
    "    textGenerated = []\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(numGenerate):\n",
    "        predictions = model(inputEval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a multinomial distribution to predict the word returned by the trainModel\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        # We pass the predicted word as the next input to the trainModel\n",
    "        # along with the previous hidden state\n",
    "        inputEval = tf.expand_dims([predicted_id], 0)\n",
    "        textGenerated.append(idx_2_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(textGenerated))\n",
    "\n",
    "generated = generate_text(\n",
    "        model=genModel, \n",
    "        start_string=\"One day three people embark togheter\", \n",
    "        char_2_idx=char2idx, \n",
    "        idx_2_char=idx2char\n",
    "    )\n",
    "\n",
    "print(generated)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
