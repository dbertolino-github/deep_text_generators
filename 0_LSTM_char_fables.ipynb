{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>ATTENTION</b>:\n",
    "<p>This entire exercise is inspired directly from a Tensorflow tutorial, click the link if you need more details about it.</p>\n",
    "<a href=\"https://www.tensorflow.org/alpha/tutorials/text/text_generation\">Tensorflow tutorial</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies \n",
    "Tensorflow background session is launched to define GPU settings and eager excecution is enabled:\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/guide/eager\">Eager execution details</a>\n",
    "\n",
    "\n",
    "In this first step we also define all global variables that will help managing redundancy:\n",
    "\n",
    "- __*SEQUENCES_LENGTH*__: length (n. of chars) of the chuncks in which the entire text will be divided in during preprocess.\n",
    "- __*NUM_GENERATE*__: numbers of characters to be generated.\n",
    "- __*EPOCHS*__: number of epohcs in which the training is divided.\n",
    "- __*BATCH_SIZE*__: number of samples after which update the wieghts.\n",
    "- __*BEDDING_DIM*__: number of neurons in the Embeddings layer.\n",
    "- __*NN_DIM*__: number of LSTM units in the networ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEQUENCES_LENGTH= 30\n",
    "NUM_GENERATE= 500\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_DIM = 1024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Aesop fables data\n",
    "The chosen dataset is a JSON file containing 147 Aesop Fables divided in sentences.\n",
    "For the availabilty, I need to to thanks this funny and interesting project on Aesop Fables which explore the connections between them using machine learning: <a href=\"https://github.com/itayniv/aesop-fables-stories\">GitHub repository</a>\n",
    "\n",
    "Here an example of how it is structured:\n",
    "```json\n",
    "{\n",
    "  \"stories\":[\n",
    "    {\n",
    "      \"number\": \"01\",\n",
    "      \"title\": \"THE WOLF AND THE KID\",\n",
    "      \"story\": [\n",
    "        \"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself.\",\n",
    "        \"So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass.\",\n",
    "        \"A little later when he lifted his head, the flock was gone.\",\n",
    "        \"He was all alone.\",\n",
    "        \"The sun was sinking.\",\n",
    "        \"Long shadows came creeping over the ground.\",\n",
    "        \"A chilly little wind came creeping with them making scary noises in the grass.\",\n",
    "        \"The Kid shivered as he thought of the terrible Wolf.\",\n",
    "        \"Then he started wildly over the field, bleating for his mother.\",\n",
    "        \"But not half-way, near a clump of trees, there was the Wolf!\",\n",
    "        \"The Kid knew there was little hope for him.\",\n",
    "        \"Please, Mr. Wolf, he said trembling, I know you are going to eat me.\",\n",
    "        \"But first please pipe me a tune, for I want to dance and be merry as long as I can.\",\n",
    "        \"The Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily.\",\n",
    "        \"Meanwhile, the flock was moving slowly homeward.\",\n",
    "        \"In the still evening air the Wolf's piping carried far.\",\n",
    "        \"The Shepherd Dogs pricked up their ears.\",\n",
    "        \"They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture.\",\n",
    "        \"The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade.\"\n",
    "      ],\n",
    "      \"moral\": \"Do not let anything turn you from your purpose.\",\n",
    "      \"characters\": []\n",
    "    }, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 fables imported.\n",
      "147 plots cleaned.\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"ain't\", \"am not\")\n",
    "    text = text.replace(\"aren't\", \"are not\")\n",
    "    text = text.replace(\"can't\", \"cannot\")\n",
    "    text = text.replace(\"can't've\", \"cannot have\")\n",
    "    text = text.replace(\"'cause\", \"because\")\n",
    "    text = text.replace(\"could've\", \"could have\")\n",
    "    text = text.replace(\"couldn't\", \"could not\")\n",
    "    text = text.replace(\"couldn't've\", \"could not have\")\n",
    "    text = text.replace(\"should've\", \"should have\")\n",
    "    text = text.replace(\"should't\", \"should not\")\n",
    "    text = text.replace(\"should't've\", \"should not have\")\n",
    "    text = text.replace(\"would've\", \"would have\")\n",
    "    text = text.replace(\"would't\", \"would not\")\n",
    "    text = text.replace(\"would't've\", \"would not have\")\n",
    "    text = text.replace(\"didn't\", \"did not\")\n",
    "    text = text.replace(\"doesn't\", \"does not\")\n",
    "    text = text.replace(\"don't\", \"do not\")\n",
    "    text = text.replace(\"hadn't\", \"had not\")\n",
    "    text = text.replace(\"hadn't've\", \"had not have\")\n",
    "    text = text.replace(\"hasn't\", \"has not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd\", \"he would\")\n",
    "    text = text.replace(\"haven't\", \"have not\")\n",
    "    text = text.replace(\"he'd've\", \"he would have\")\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\"'t\", \"\")\n",
    "    text = text.replace(\"'ve\", \"\")\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    text = text.replace(\"?\", \" ? \")\n",
    "    text = text.replace(\";\", \" ; \")\n",
    "    text = text.replace(\":\", \" : \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = text.replace(\"´\", \"\")\n",
    "    text = text.replace(\"‘\", \"\")\n",
    "    text = text.replace(\"’\", \"\")\n",
    "    text = text.replace(\"“\", \"\")\n",
    "    text = text.replace(\"”\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"–\", \"\")\n",
    "    text = text.replace(\"—\", \"\")\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\",\"\")\n",
    "    text = text.replace(\"{\",\"\")\n",
    "    text = text.replace(\"}\", \"\")\n",
    "    text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.replace(\")\", \"\")\n",
    "    text = text.replace(\"$\", \"\")\n",
    "    text = text.replace(\"+\", \"\")\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"%\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    return text\n",
    "\n",
    "try:\n",
    "    \n",
    "    fables = []\n",
    "    fablesText = ''\n",
    "    dirname = os.path.abspath('')\n",
    "    filepath = os.path.join(dirname, 'input_data/aesopFables.json')\n",
    "\n",
    "    with open(filepath) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        for p in data['stories']:\n",
    "            fables.append(' '.join(p['story']))\n",
    "            \n",
    "    print('{} fables imported.'.format(len(fables)))\n",
    "    \n",
    "    cleanedFables = []\n",
    "    for f in fables:\n",
    "        cleaned = clean(f)\n",
    "        cleanedFables.append(cleaned)\n",
    "        fablesText += ' ' + cleaned + '\\n'\n",
    "    \n",
    "    print('{} plots cleaned.'.format(len(cleanedFables)))\n",
    "    \n",
    "except IOError:\n",
    "    \n",
    "    sys.exit('Cannot find data!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to investigate on fables max length to better decided preprocess hyperparamateres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = 0\n",
    "for f in cleanedFables:\n",
    "    l = len(f)\n",
    "    if l > maxLen: maxLen = l\n",
    "\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Vocabulary\n",
    "The vocabulary is saved as: \n",
    "- a __numpy array__ to map each encoding to the right character\n",
    "- a __dictionary__ to map each character to its encoding number \n",
    "\n",
    "We also create a __textAsInt__ variable that contains all fables text encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "34 unique characters\n",
      "\n",
      "' there was once a li' ---- characters mapped to int ---- > [ 1 27 15 12 25 12  1 30  8 26  1 22 21 10 12  1  8  1 19 16]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(fablesText))\n",
    "print(vocabulary)\n",
    "vocab_size = len(vocabulary)\n",
    "print ('{} unique characters\\n'.format(len(vocabulary)))\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocabulary)}\n",
    "idx2char = np.array(vocabulary)\n",
    "textAsInt = np.array([char2idx[c] for c in fablesText])\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(fablesText[:20]), textAsInt[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess text\n",
    "\n",
    "Given a character, or a sequence of characters, what is the most probable next character? <br/>\n",
    "This is the task we're training the model to perform, the input to the model will be a sequence of characters, and we train the model to predict the following character at each time step. \n",
    "\n",
    "We're going to divide the text into sequences, each input sequence will contain __SEQUENCES_LENGTH__ number of characters from the text. For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "For example, say SEQUENCES_LENGTH is 4 and our text is \"Hello\". \n",
    "- Input: \"Hell\"\n",
    "- Target:\"ello\".\n",
    "\n",
    "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples per Epoch: 4309\n",
      "Steps per Epoch: 134\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      "\n",
      "\n",
      "' there was once a little kid wh'\n",
      "'ose growing horns made him thin'\n",
      "'k he was a grownup billy goat a'\n",
      "'nd able to take care of himself'\n",
      "' .  so one evening when the flo'\n",
      "\n",
      "Input data:  ' there was once a little kid w'\n",
      "Target data: 'there was once a little kid wh'\n",
      "Step    0\n",
      "  input: 1 (' ')\n",
      "  expected output: 27 ('t')\n",
      "Step    1\n",
      "  input: 27 ('t')\n",
      "  expected output: 15 ('h')\n",
      "Step    2\n",
      "  input: 15 ('h')\n",
      "  expected output: 12 ('e')\n",
      "Step    3\n",
      "  input: 12 ('e')\n",
      "  expected output: 25 ('r')\n",
      "Step    4\n",
      "  input: 25 ('r')\n",
      "  expected output: 12 ('e')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 30), (32, 30)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    inputText = chunk[:-1]\n",
    "    targetText = chunk[1:]\n",
    "    return inputText, targetText\n",
    "\n",
    "# Create training examples and targets\n",
    "examplesPerEpoch = len(fablesText) // SEQUENCES_LENGTH\n",
    "stepsPerEpoch = examplesPerEpoch // BATCH_SIZE\n",
    "print('Examples per Epoch: {}'.format(examplesPerEpoch))\n",
    "print('Steps per Epoch: {}'.format(stepsPerEpoch))\n",
    "\n",
    "charDataset = tf.data.Dataset.from_tensor_slices(textAsInt)\n",
    "for i in charDataset.take(10):\n",
    "    print(idx2char[i.numpy()])\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "sequences = charDataset.batch(SEQUENCES_LENGTH+1, drop_remainder=True)#The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('\\nInput data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "        print(\"Step {:4d}\".format(i))\n",
    "        print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "        print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the model\n",
    "The model will be a simple Neural Network composed by:\n",
    "- Embeddings layer \n",
    "- Recurrent Layer (Long Short Memory Networks)\n",
    "- Dense layer with vocabulary size dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 30, 34) # (batch_size, sequence_length, vocab_size)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 128)           4352      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (32, None, 1024)          4726784   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 34)            34850     \n",
      "=================================================================\n",
      "Total params: 4,765,986\n",
      "Trainable params: 4,765,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM \n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "trainModel = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=RNN_DIM,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = trainModel(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "trainModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model\n",
    "We train the model and save its weigths in .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "134/134 [==============================] - 3s 22ms/step - loss: 2.4686\n",
      "Epoch 2/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.9239\n",
      "Epoch 3/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.7296\n",
      "Epoch 4/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.5861\n",
      "Epoch 5/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.4764\n",
      "Epoch 6/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.3825\n",
      "Epoch 7/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.3105\n",
      "Epoch 8/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.2358\n",
      "Epoch 9/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.1646\n",
      "Epoch 10/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.0961\n",
      "Epoch 11/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 1.0274\n",
      "Epoch 12/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.9608\n",
      "Epoch 13/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.8977\n",
      "Epoch 14/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.8402\n",
      "Epoch 15/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.7858\n",
      "Epoch 16/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.7430\n",
      "Epoch 17/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6933\n",
      "Epoch 18/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6613\n",
      "Epoch 19/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6301\n",
      "Epoch 20/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.6036\n",
      "Epoch 21/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5818\n",
      "Epoch 22/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5658\n",
      "Epoch 23/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5470\n",
      "Epoch 24/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5320\n",
      "Epoch 25/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5151\n",
      "Epoch 26/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.5057\n",
      "Epoch 27/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4984\n",
      "Epoch 28/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4874\n",
      "Epoch 29/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4846\n",
      "Epoch 30/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4764\n",
      "Epoch 31/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4709\n",
      "Epoch 32/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4624\n",
      "Epoch 33/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.4595\n",
      "Epoch 34/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4539\n",
      "Epoch 35/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4483\n",
      "Epoch 36/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4436\n",
      "Epoch 37/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4384\n",
      "Epoch 38/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4395\n",
      "Epoch 39/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4318\n",
      "Epoch 40/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4335\n",
      "Epoch 41/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4292\n",
      "Epoch 42/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4255\n",
      "Epoch 43/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.4241\n",
      "Epoch 44/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4213\n",
      "Epoch 45/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4171\n",
      "Epoch 46/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4151\n",
      "Epoch 47/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.4137\n",
      "Epoch 48/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4131\n",
      "Epoch 49/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4095\n",
      "Epoch 50/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4071\n",
      "Epoch 51/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4036\n",
      "Epoch 52/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.4039\n",
      "Epoch 53/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4005\n",
      "Epoch 54/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3996\n",
      "Epoch 55/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.4005\n",
      "Epoch 56/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3958\n",
      "Epoch 57/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3978\n",
      "Epoch 58/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3968\n",
      "Epoch 59/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3961\n",
      "Epoch 60/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3929\n",
      "Epoch 61/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3919\n",
      "Epoch 62/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3881\n",
      "Epoch 63/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3876\n",
      "Epoch 64/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3861\n",
      "Epoch 65/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3830\n",
      "Epoch 66/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3850\n",
      "Epoch 67/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3831\n",
      "Epoch 68/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3803\n",
      "Epoch 69/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3783\n",
      "Epoch 70/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3767\n",
      "Epoch 71/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3777\n",
      "Epoch 72/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3784\n",
      "Epoch 73/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3787\n",
      "Epoch 74/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3787\n",
      "Epoch 75/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3744\n",
      "Epoch 76/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3730\n",
      "Epoch 77/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3754\n",
      "Epoch 78/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3724\n",
      "Epoch 79/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3684\n",
      "Epoch 80/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3719\n",
      "Epoch 81/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3713\n",
      "Epoch 82/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3721\n",
      "Epoch 83/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3691\n",
      "Epoch 84/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3723\n",
      "Epoch 85/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3688\n",
      "Epoch 86/100\n",
      "134/134 [==============================] - 2s 18ms/step - loss: 0.3677\n",
      "Epoch 87/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3676\n",
      "Epoch 88/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3652\n",
      "Epoch 89/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3679\n",
      "Epoch 90/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3673\n",
      "Epoch 91/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3645\n",
      "Epoch 92/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3641\n",
      "Epoch 93/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3635\n",
      "Epoch 94/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3623\n",
      "Epoch 95/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3624\n",
      "Epoch 96/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3619\n",
      "Epoch 97/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3609\n",
      "Epoch 98/100\n",
      "134/134 [==============================] - 3s 19ms/step - loss: 0.3627\n",
      "Epoch 99/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3594\n",
      "Epoch 100/100\n",
      "134/134 [==============================] - 2s 17ms/step - loss: 0.3602\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "trainModel.compile(\n",
    "      optimizer = tf.train.AdamOptimizer(),\n",
    "      loss = loss)\n",
    "\n",
    "trainModel.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=stepsPerEpoch)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_char_fables_{}_{}_{}_{}_{}_.h5'.format(\n",
    "    EPOCHS, \n",
    "    SEQUENCES_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    RNN_DIM)\n",
    ")\n",
    "trainModel.save_weights(weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation model\n",
    "The generation model is the same used in training but with a __BATH_SIZE__ equal to 1 so that the model can digest one sample at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 128)            4352      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (1, None, 1024)           4726784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 34)             34850     \n",
      "=================================================================\n",
      "Total params: 4,765,986\n",
      "Trainable params: 4,765,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.layers.CuDNNLSTM\n",
    "\n",
    "genModel = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=RNN_DIM,\n",
    "  batch_size=1)\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "weightsPath = os.path.join(dirname, 'models/rnn_char_fables_{}_{}_{}_{}_{}_.h5'.format(\n",
    "    EPOCHS, \n",
    "    SEQUENCES_LENGTH, \n",
    "    BATCH_SIZE, \n",
    "    EMBEDDING_DIM,\n",
    "    RNN_DIM)\n",
    ")\n",
    "genModel.load_weights(weightsPath)\n",
    "genModel.build(tf.TensorShape([1, None]))\n",
    "genModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate text\n",
    "In order to generate a sentence with a fixed dimensionality, the following generation loop is implemented:\n",
    "\n",
    "- It Chooses a start string, initializes the RNN state and sets the number of characters to generate.\n",
    "- It gets the prediction distribution of the next character using the start string and the RNN state.\n",
    "- It uses a multinomial distribution to calculate the index of the predicted character and then it uses this predicted character as our next input to the model.\n",
    "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
    "\n",
    "<img src=\"images/generation_loop.png\" alt=\"Generation Loop\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-f8d03d815d18>:26: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "there was once a little bear him ,  and the king of beasts is like wax in the ground roon with them ,  and the snakes kindly appearance ,  he bears .  so he about to d .  just then the cat let go a good lesson learned . \n",
      " a friving her good boasted now the cranes were going to straid and strck .  besides ,  the animals were carried one of them .  it bear whire the lion had little happen i cannot tell you how gecorners .  i see !  mother mole saw sure i can soon gnaw this stalks out of the ground with  creature .  a thirsty\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, char_2_idx, idx_2_char):\n",
    "    '''\n",
    "    '''\n",
    "    # Evaluation step (generating text using the learned weights)\n",
    "    # Number of characters to generate\n",
    "    numGenerate = NUM_GENERATE\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    start_string = clean(start_string) \n",
    "    inputEval = [char_2_idx[s] for s in start_string]\n",
    "    inputEval = tf.expand_dims(inputEval, 0)\n",
    "    # Empty string to store our results\n",
    "    textGenerated = []\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(numGenerate):\n",
    "        predictions = model(inputEval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a multinomial distribution to predict the word returned by the trainModel\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        # We pass the predicted word as the next input to the trainModel\n",
    "        # along with the previous hidden state\n",
    "        inputEval = tf.expand_dims([predicted_id], 0)\n",
    "        textGenerated.append(idx_2_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(textGenerated))\n",
    "\n",
    "generated = generate_text(\n",
    "        model=genModel, \n",
    "        start_string=\"There was once a little Bear\", \n",
    "        char_2_idx=char2idx, \n",
    "        idx_2_char=idx2char\n",
    "    )\n",
    "\n",
    "print(generated)\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
